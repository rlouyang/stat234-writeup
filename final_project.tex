\documentclass[11pt]{article}
\usepackage{final_project}
\usepackage{todonotes}

\usepackage{longtable}
\usepackage{pgffor}

\usepackage[utf8]{inputenc} 
% \doublespacing

\newcommand{\cp}{\texttt{CartPole}}
\newcommand{\ab}{\texttt{Acrobot}}
\newcommand{\mc}{\texttt{MountainCar}}

\title{State Space Reduction in Deep Q-Networks} 
\author{Michael Ge, Richard Ouyang} 
\date{April 24, 2018}

\begin{document}

\maketitle

\begin{abstract}
    Deep convolutional neural networks have become a popular approach to estimating Q-value functions in reinforcement learning problems. These deep Q-networks take in entire images as network inputs, often resulting in large state spaces and long learning times. In this paper, we explore the use of principal component analysis to reduce the state space of network inputs. After testing multiple network configurations, we determine that a reduction in uninformative state features through PCA helps improve the performance of deep reinforcement learning.
\end{abstract}

\section{Introduction}

Current methods of Q-learning with deep networks are inhibited by the
large state space inherent in processing images. The state-of-the-art
methods used in recent papers \cite{mnih2013playing, mnih2015human, van2016deep} require often infeasible
amounts of computational power, time, and data. Although convolutional
neural networks theoretically reduce the learning time by restricting
the size of the network and accounting for structural information in
the data, it is still difficult to learn a good policy in a time- and
data-efficient manner. We experiment with reducing state space
dimensionality required by deep Q-networks (DQNs) by applying
principal component analysis (PCA) to improve learned policies and
training times on a variety of OpenAI games.

In addition, we provide an extensible and easy-to-use software
framework to test various types of agents and Q-networks, even for
games not tested in this paper (such as Atari games).

\section{Background}

\subsection{MDP Overview}

We will briefly describe the general Markov Decision Process (MDP)
framework. Define MDP to be:

\begin{itemize}
\item $\mathcal{S}$, the set of states. In our usage, each state $s$
is a transformation of the games' screens, which are matrices of pixel
values.
  
\item $\mathcal{A}$, the set of actions. The available actions $a$
depend on the setting of the game. In the problems of interest, we
consider games with discrete, relatively simple action spaces.
  
\item $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, the reward
function. A reward $r_t(s_t, a_t)$ is given to the agent after an
action $a_t$ is taken at state $s_t$. The reward is often a complex
function. If we knew the reward function, we would easily know which
action to take at any state $s_t$ and timestep $t$.
  
\item $p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to
\mathbb{R}$, the transition probabilities. The agent moves from $s_t$
to $s_{t+1}$ after taking action $a$ with probability $p(s_{t+1} |s_t,
a_t)$. In our games, transitions are deterministic, so probabilities
are either $0$ or $1$.
  
\item $\gamma \in [0, 1]$, the discount factor. $\gamma$ discounts
  future rewards at a constant, compounded rate.

  In reinforcement learning, we seek to learn a policy
  $\pi: \mathcal{S} \to \mathcal{A}$ that maximizes the expected
  discounted sum of future rewards at a given
  state:
  $$R_t = \mathbb{E}\left[\sum_{i=t}^\infty \gamma^{i-t} r_{i}|s_{t-1}\right]$$

  We define the Q-function under a policy $\pi$ as the expected
  discounted sum of future rewards given a state and desired action:
  $$Q^\pi(s_t, a_t) = \mathbb{E}\left[R_t | s_{t-1}, a_{t-1} \right]$$

  The optimal Q-function has been proven to satisfy the Bellman equation:

  $$Q^\star (s, a) = r(s, a) + \gamma \max_{a' \in \mathcal{A}} Q^\star(s', a')$$
\end{itemize}


\subsection{Deep Q-Networks}

We now briefly discuss the application of deep neural networks to reinforcement learning. The most common use of deep neural networks is to model the Q-function. These deep Q-networks (DQNs) take in a transformed image input $s$, passes it through multiple layers, and returns a vector consisting of the estimated $Q(s, a)$ for all available actions $a \in \mathcal{A}$. 

Current state-of-the-art models \cite{mnih2013playing, mnih2015human} use deep convolutional neural networks (CNNs) as models for the Q-function; these DQNs capitalize on the structure of image data -- for example, correlations between nearby pixels -- to make Q estimates. Unfortunately, DQNs typically require a large number of parameters, thus consuming large amounts of computational resources, both in terms of time and space.

Double DQNs (DDQNs) \cite{van2016deep} are very similar to regular DQNs. However, instead of using the same network to obtain the target value for minibatch updates as in a regular DQN, the DDQN randomly selects one of the two networks to update and uses the other network to obtain the target value.

\todo{maybe math}

\subsection{Principal Component Analysis}

Principal component analysis (PCA) is a dimensionality reduction technique that linearly projects the original features into a smaller dimension. PCA chooses the axes with the most variance to construct this smaller space. The resulting features have zero correlation with one another. In interpreting images, PCA keeps transformations of the most variable pixels, eliminating the information contained in pixels with very little variance.

PCA works as follows: \todo{maybe math}

\cite{kishima2013reduction} tried a version of this

\subsection{OpenAI Gym}

OpenAI Gym \cite{brockman2016openai} offers a set of many standard games on which to test reinforcement learning algorithms. We tested our algorithms, which learn on pixel images, on the following games. See Figure \ref{fig:game_images} for sample images of each game.

\begin{description}
    \item[\href{https://gym.openai.com/envs/CartPole-v0/}{CartPole-v0}] \cp~is the classic reinforcement learning environment. At every time step, the agent receives a reward of $1$ and chooses to push the cart either right or left. The game ends when the pole falls to an angle greater than 15 degrees from vertical or when the cart moves off the screen. 
    \item[\href{https://gym.openai.com/envs/Acrobot-v1/}{Acrobot-v1}] \ab~is a reinforcement learning game consisting of a double pendulum. The agent can only apply force to the connecting joint (left, right, or no movement) and must swing the pendulum so that the tip of the pendulum reaches a certain height. The agent receives a reward of $-1$ for every time step the goal is not reached.
    \item[\href{https://gym.openai.com/envs/MountainCar-v0/}{MountainCar-v0}] \mc~is a reinforcement learning game in which the agent tries to drive a car up a hill. The car is not strong enough to climb the hill itself and thus must learn to use gravity to achieve the necessary speed. At every time step, the three movements available are accelerating to the right, accelerating to the left, and doing nothing. The agent receives a reward of $-1$ for every time step the goal is not reached. 
\end{description}

\begin{figure}[!ht]
\foreach \game in {CartPole, Acrobot, MountainCar}
{
    \subfloat[Sample original image of the \game~game screen. \label{subfig:\game_original}]{%
        \includegraphics[width=0.35\textwidth]{game_images/\game_original.pdf}
    }
    \hfill
    \subfloat[Sample processed image of the \game~game screen.\label{subfig:\game_processed}]{%
        \includegraphics[width=0.35\textwidth]{game_images/\game_processed.pdf}
    }
}
\caption{Original and processed images from the three games discussed in this paper. The processed images were created by grayscaling the original images and resizing from $600 \times 400$ pixels to $80 \times 80$ pixels. The resulting feature space is then $\mathbb{R}^{6400}$.}
\label{fig:game_images}
\end{figure}

\subsection{PyTorch}

PyTorch \cite{paszke2017pytorch} is a Python framework for constructing neural network architectures. Although PyTorch is still in its early stages of development, it has several advantages compared to other neural network libraries such as TensorFlow and Keras. Namely, improvements include a clean extension of the common Python package \texttt{NumPy}, deep integration with Python, and efficient memory usage. 

\section{Methods}

\subsection{Initial Setup}

We replicated the original deep Q-network papers \cite{mnih2013playing, mnih2015human} in Python, using PyTorch \cite{paszke2017pytorch} for the neural network architecture and OpenAI Gym \cite{brockman2016openai} to test and compare our algorithms on different games. 

There are quite a few steps to get started to replicate our procedure, including changing some of the source code in OpenAI Gym. For the sake of brevity, these instructions are included in Appendix \ref{app:getting_started}. To run our experiments, we used the Odyssey research computing cluster to run computationally intensive jobs.

\subsection{Image Preprocessing}

In this project, our goal was to learn the optimal policy for the games without using parametric data -- for example, the pole angle in \cp. Instead, we focused more on image processing rather than using the state information given by the Gym environment.

At each time step, we obtained the RGB (color) mapping of the game screen, resized the image to $80 \times 80$ pixels (rectangular images were squashed into a square), and converted the image to grayscale. We then took the difference between the current timestep's image and the last timestep's image as the state to input into our DQN. 

\subsection{Model Structures}
\label{subsec:model_structure}

In our experiments, we used four different types of models, each of which includes the original and double network variants: 

\begin{description}
    \item[(D)DQCNN] The original model \cite{mnih2013playing, mnih2015human} and variants \cite{van2016deep}. These models convolve images using no extra state-space reduction techniques like PCA.
    
    \item[(D)DQN-PCA] These models use a database of 1000 states as training data for PCA and project the pixel features of new states onto the subspace that captures 99\% of the variance. The results is a one-dimensional vector with varying lengths based on the game (100 to 500 features). This vector is passed into a conventional (not convolutional) deep neural network to predict the Q-value.
    
    \item[(D)DQCNN-PCA] These models perform PCA as above, but inverts the PCA transformation in order to convert each image back to its original space. The result is again an $80 \times 80$ image, which is then input into a model with the same network structure as the original (D)DQN.
    
    \item[(D)DQCNN-PCA-Mini] Out of curiosity, we included one additional model, which takes the one-dimensional vector obtained from the PCA transformation and reshapes it into a square (zero padding if necessary). We then convolved the resulting ``image'' through a smaller neural network to predict the Q-value for each state. Although PCA strips correlation between nearby pixels, we were interested in investigating whether the reduced dimensionality preserved some structure between nearby values.
\end{description}

For the (D)DQCNN and (D)DQCNN-PCA models, we kept our network structure similar to \cite{mnih2013playing} and \cite{mnih2015human} by using three hidden layers in each network. These DQNs take in an $80 \times 80$ grayscale image. Each hidden layer in the network convolves the previous layer's output into multiple filters, followed by a batch normalization and a leaky ReLU with $\alpha = 0.001$ to prevent dead nodes. The first layer convolves $8$ $8 \times 8$ filters with stride $4$; the second layer convolves $16$ $4 \times 4$ filters with stride $2$; the final hidden layer convolves $16$ $4 \times 4$ filters, this time with stride $1$. A max pooling with kernel size $2$ is then applied to reduce the dimensionality of the output. The final layer is fully connected and maps linearly to the number of actions available. For instance, since \cp~contains two actions at any time, the output of the neural network contains two nodes. This structure saves time when forwarding through the network, since a single forward pass for a given state $s$ gives $\hat{Q}_\text{network}(s, a)$ for all $a \in \mathcal{A}$.

To keep the models relatively similar in size, we kept the number of parameters for each model as close as possible. For the (D)DQN-PCA model, we tried a variety of network architectures, varying the number of layers as well as the number of nodes in each layer. 

For the (D)DQCNN-PCA-Mini model, we kept the rough network structure the same as our original models. However, to account for the smaller image size, we reduced the kernel size to 4 and the stride to 1 for all layers.

\subsection{Hyperparameters}

We tested many configurations of hyperparameters, varying the following: 

\begin{description}
    \item[Model] One of the Q-function models mentioned in Section \ref{subsec:model_structure}.
    
    \item[Frame skip] The number of time steps for which the same action is repeated. To speed up learning, we used this method described in \cite{mnih2013playing, mnih2015human}. When an action is selected, that action is repeated for the next $k$ frames, since selecting an action and training takes much more time than rendering an additional step of the game. We chose $k = 3$ since $k = 4$, which was suggested in \cite{mnih2013playing, mnih2015human}, empirically performed worse in \cp. 
    
    \item[Update frequency] The number of distinct action selections between each minibatch update. This value is typically set to $4$. This technique speeds up the training even further, since rendering the environment is far less costly than forwarding through the network. This has been used in other papers \cite{mnih2013playing, mnih2015human, van2016deep}.
    
    \item[Number of minibatch training steps] The length of time the model should be trained, expressed in terms of the number of minibatch training steps. While state-of-the-art research uses up to 10 million frames, our resources restrict us to values between 10 thousand and 100 thousand.
    
    \item[Replay memory size] The maximum number of transitions -- each of which is a $(s, a, r, s')$ tuple -- stored in the replay memory. The replay memory is used to reduce correlation between transitions used in each minibatch update \cite{mnih2013playing, mnih2015human}.
    
    \item[Target update] If the model uses a target network \cite{mnih2015human}, this represents the number of trains between each target network update. Each update involves setting the target network parameters to the main network parameters.
    
    \item[Learning rate] The (initial) learning rate used by the optimizer. A higher rate results in a model that learns faster but may diverge.
    
    \item[Learning rate annealing] Whether the learning rate is annealed to a smaller value. Learning rate annealing helps decrease the likelihood of policy divergence. The learning rate anneals according to the equation $$\alpha_t = \alpha_\text{initial} \max\left(\exp\left(-\frac{t}{\lambda_\alpha}\right), \frac{0.0005}{\alpha_\text{initial}}\right),$$ where $t$ is the number of minibatch training updates that have already occurred, and $\lambda$ is the decay rate, so that the learning rate anneals exponentially from its initial value to a final value of $0.0005$.
    
    \item[Batch size] The minibatch size, or the number of transitions used in each training update. The two values tested were $32$ and $128$.
    
    \item[Loss function] The loss function used in the model. The Huber loss (defined as the $\ell_2$ norm for losses greated than 1 and the $\ell_1$ norm otherwise) and the mean squared error (MSE) loss were the two loss functions considered.
    
    \item[Regularization] The weight decay in the optimizer. This value typically ranges from $0$ to $1$ and is used to prevent overfitting in our networks.
    
    \item[Agent action selection] We used an $\epsilon$-greedy agent, annealing $\epsilon$ from $\epsilon_\text{initial} = 1$ to $\epsilon_\text{final} = 0.1$ according to the equation $$\epsilon_t = \epsilon_\text{final} + (\epsilon_\text{initial} - \epsilon_\text{final}) \exp\left(-\frac{t}{\lambda_\epsilon}\right),$$ where $t$ is the number of times the agent has already selected an action, and $\lambda$ is the decay rate. 
\end{description}


\subsection{Implementation Details}

\subsubsection{Model Implementation}

The code is structured such that the user can pass a model, agent, game, and a large selection of hyperparameters into \texttt{main.py}. During the script execution, statistics are logged for later analysis. 

Within our \path{utils/} folder, we implemented several scripts to aid in the data analysis process, some of which we will briefly discuss here. 

\begin{itemize}
    \item \texttt{Logger.py} is a general logging utility for storing statistics in files. 
    \item \texttt{PCA.py} contains a PCA class whose objects can train on inputs of states, return the transformed features, and invert already-transformed features back into the original image space. 
    \item \texttt{ReplayMemory.py} contains a class that stores and quickly samples transitions for minibatch updates. 
    \item \path{get_notes_and_stats.py} parses our raw data into an easy-to-read format, providing summary statistics, hyperparameters, and metadata about each job. 
    \item \path{plot_data.py} plots raw data as well as running means over episodes and training updates. 
    \item \path{save_states.py} runs a random policy for each game, saving a large set of states on which PCA can later be performed. 
\end{itemize}
 
Our \path{automate_run} folder contains the following bash scripts for running jobs on the computing cluster:

\begin{itemize}
    \item \path{gen_slurms.sh} and \path{search_nets.sh} iterate over a given hyperparameter space and submit batch jobs for each hyperparameter combination in parallel.
    \item \path{view_plots.sh} is a utility that allows the user to view the rewards of each game, providing a preliminary categorization of model performance. 
\end{itemize}

Although we did have access to a computing cluster, we did not have access to GPUs, which significantly speed up learning for neural networks. We also had limited time to run our experiments, preventing us from running the 10 million frames per game prescribed in the literature \cite{mnih2013playing, mnih2015human}. This was the motivation for our efforts to improve learning by reducing the state space.

\section{Results}

The full set of raw data, metadata, and plots are available at this \href{https://drive.google.com/drive/folders/15gt9bv0kPBCHnJyW_RzsYQygeMaK1uCf?usp=sharing}{link} (warning: approximately 700 MB).

\subsection{Random Policy}

As a baseline, we ran a random policy on each game. The average rewards and durations for the three games are available in Table \ref{tab:random_rewards}. With this initial goal in mind, we proceeded to test different model architectures and hyperparameter settings to achieve the best possible empirical results over all three games.

\begin{figure}[!ht]
\foreach \game in {CartPole, Acrobot, MountainCar}
{
    \subfloat[Rewards for a random policy in the \game~game. \label{subfig:\game_NoTraining_Random_rewards}]{%
        \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_rewards.pdf}
    }
    \hfill
    \subfloat[Durations for a random policy in the \game~game.\label{subfig:\game_NoTraining_Random_duraions}]{%
        \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_durations.pdf}
    }
}
\caption{Baseline results from several classic control games from OpenAI Gym under a random policy and model.}
\label{fig:random}
\end{figure}

\begin{table}[!htbp]
    \centering
    \begin{tabular}{c|cc}
        \toprule
        Game & Average Duration & Average Reward \\ \midrule
        CartPole-v0 & 18.441 & 18.441 \\
        Acrobot-v1 & 599.73 & -598.73 \\
        MountainCar-v0 & 2717.77 & -2717.77 \\
        \bottomrule
    \end{tabular}
    \caption{Average rewards and durations under a random policy for the three games.}
    \label{tab:random_rewards}
\end{table}

% \subsection{Initial Exploration}

% Although \cite{mnih2013playing, mnih2015human} used a learning rate of $0.00025$, we found that learning was far too slow for our purposes and instead opted for a larger learning rate. We began with a learning rate of $0.1$, but this was far too large: CartPole would learn to durations of around 200 time steps within a few dozen iterations but would quickly diverge. The other two games also did not converge. Significant improvements occurred after reducing to a learning rate of about $0.001$. 

% We used

% \begin{itemize}
%     \item \cp~learning rate 001
% \end{itemize}

\subsection{Hyperparameter Tuning Results}

% we should make note of the fact that the statistics are the stats of the latter half of the n_trains iterations

All summary statistics 

\subsubsection{First Grid Search}

In order to establish a solid set of hyperparameters to which PCA variants of DQNs could be compared, we began our experiments with an initial grid search over the hyperparameters of interest for $10,000$ training steps to eliminate poorly performing hyperparameter settings. We decided to keep the frame skip, update frequency, replay memory size, and agent action selection hyperparameters constant for testing, since grid search sizes increase exponentially in the number of parameters, and varying these settings would vastly increase simulation time and space consumption. Table \ref{tab:grid_search_v1_10k} contains the results of the top fifteen models for each game ordered by performance.

\begin{table}[!htbp]
  \footnotesize
  \centering
  \input{csvs/grid_search_v1_10k.tex}
  
  \caption{Top fifteen parameter-tuned means per game between DDQN-GS and DQN-GS for $10,000$ training steps.}
  \label{tab:grid_search_v1_10k}
\end{table}

From these results, we already find that our best hyperparameter configurations are beating the random baseline. \mc~was rather difficult to learn, as the final policy seems to depend heavily on the random initialization. With suboptimal hyperparameters, the episodes under a poorly learned policy can extend to tens of thousands of time steps, far more than a random policy. This particular vulnerability to divergence likely occurs because all transitions entered into the replay memory have $r = -1$, creating difficulties in learning. \cp~roughly doubles the duration of the random policy, though the performance has significantly high variance in all models. Finally, \ab~seems to have been learned quite quickly, reaching optimal performance within the 10,000 iterations.

From the results, we observe strong performance by DDQNs over DQNs using a Huber loss function and RMSProp. The Adam optimizer failed to learn in almost every simulation. Weight decay yielded mixed results, likely due to the fact that we only allowed the models to run for 10,000 training steps. Additionally, the effectiveness of the learning rate parameters varied significantly with the game; for example, \cp~learns better with a significantly higher learning rate than the other two games. We also saw that including a learning rate annealing slightly improved the overall performance of the algorithms, so we decided to keep the annealing. In theory, a learning rate annealing ought to improve the stability of the model after the model has had time to learn a strong policy. Not displayed in the data is the surprising fact that DQNs with an enabled target update performed significantly worse than DQNs without a target update in that they failed to reach any convergence at all, so we also decided to remove target update functionality for future experiements. With these observations in mind, we proceeded to test another batch of models for a longer period. 
 

\subsubsection{50k Grid Search}
 
 Setting the optimizer, loss function, and target update to RMSProp, Huber loss, and False, respectively, we performed a second grid search over 50,000 training iterations to tune the learning rate and weight decay. The results can be found in \ref{tab:grid_search_v2_50k}.

\begin{table}[!htbp]
  \footnotesize
  \centering
  \input{csvs/grid_search_v2_50k.tex}
  
  \caption{Huber Loss, annealing, no target update, RMSProp, $50,000$ training iterations}
  \label{tab:grid_search_v2_50k}
\end{table}

\mc~and \ab~continued to perform well with more training time for the best models, though we begin to see evidence of high variance in performance for these games. Both had the worst models scor

\begin{table}[!htbp]
  \footnotesize
  \centering
  \input{csvs/grid_search_v3_100k.tex}
  
  \caption{DDQN-GS, Huber Loss, annealing, 0.1 weight decay for $100,000$ training iterations.}
  \label{tab:grid_search_v3_100k}
\end{table}

\begin{table}[!htbp]
  \footnotesize
  \centering
  \input{csvs/model_selection_ddqn_pca_4.tex}
  
  \caption{DDQN-PCA, Huber Loss, annealing, 0.1 weight decay for $100,000$ training iterations.}
  \label{tab:model_selection_ddqn_pca_4}
\end{table}

\begin{table}[!htbp]
  \centering
  \input{csvs/final_v1_100k.tex}
  
  \caption{Model comparison with final hyperparameters over $100,000$ training iterations.}
  \label{tab:final_v1_100k}
\end{table}

Our final hyperparameters, used in the rest of the paper, are available in Table \ref{tab:grid_search_hyperparameters}.

\begin{table}[!htbp]
    \centering
    \begin{tabular}{c|c}
        \toprule
        Hyperparameter & Value \\ \midrule
        Model & DDQN \\
        Frame skip & $3$ \\
        Update frequency & $4$ \\
        Training updates & $100000$ \\
        Replay memory size & $10000$ \\
        Target update & N/A \\
        Learning rate & $0.001$ \\
        Learning rate annealing & Yes \\
        Batch size & $128$ \\
        Loss function & Huber loss \\
        Regularization & $0.1$ \\
        \bottomrule
    \end{tabular}
    \caption{The hyperparameters found by our grid search and used for the PCA variants of the (D)DQN.}
    \label{tab:grid_search_hyperparameters}
\end{table}

\subsection{PCA Networks}

See Figures \ref{fig:CartPole_DDQN-GS_rewards_losses}--\ref{fig:MountainCar_DDQCNN-PCA-Mini_rewards_losses} for a comparison of the PCA networks tested. These model comparison results generalized quite well to other hyperparameter settings.

\foreach \game in {CartPole, Acrobot, MountainCar}
{
    \foreach \model in {DDQN-GS, DDQN-PCA, DDQCNN-PCA, DDQCNN-PCA-Mini}
    {
        \begin{figure}[!ht]
            \subfloat[Rewards for the \model~model in the \game~game. \label{subfig:\game_\model_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_rewards_cpu.pdf}
            }
            \hfill
            \subfloat[Losses for the \model~model in the \game~game. \label{subfig:\game_\model_EpsilonGreedy_losses_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_losses_cpu.pdf}
            }
            \caption{Rewards and losses for the \model~model in the \game~game.}
            \label{fig:\game_\model_rewards_losses}
        \end{figure}
    }
}

We performed several grid searches to find hyperparameters that worked for all three games and to get a general feel for the parameter space. (we found that CartPole preferred a larger learning rate)


In order to find hyperparameters that worked for all three games, 

put grid search results here

Target network is bad etc.

comment on how the loss isn't converging

sample Q values don't really make sense, especially for \cp
 
MSE was bad compared to Huber - unstable

Adam, even though it's a popular optimizer, was bad (not included; used RMSProp instead)

learning rates differ for different games (\cp~wants a higher learning rate, for instance)

in order to get these results, we did some inspection and regressed means on categorical variables to see which hyperparameters were good

including weight decay and annealing help with stability

make sure to mention that 50 is a good average

None of the games performed well with DDQN-PCA, which simply inputs the reduced state space through a conventional deep neural network. This is perhaps 

\todo{talk about results here\dots}

\section{Discussion}

\section{Conclusion}

\subsection{Further Directions}

Consider the same for Atari games and others, as long as they have discrete actions. Also try to extend to games with continuous actions with a different type of network architecture. Perhaps a convolutional neural network linked with a network in which actions are the inputs?

We had severely limited computational power and time, so extending the paper using more varied learning rates and annealing schedules could offer better performance. Also computing using GPUs (which were unfortunately not available) rather than CPUs would have been great.

Adding Hindsight Experience Replay to \mc

\newpage
\bibliography{final_project}{}
\bibliographystyle{plain}

\newpage

\appendix

\section{Getting Started}
\label{app:getting_started}

\begin{enumerate}
    \item Download/install Anaconda
    
    \item Clone this paper's GitHub repository: \url{https://github.com/hahakumquat/stat234-project}
    
    \item Inside the directory containing the repository, create the \texttt{conda} environment, which consists of all the necessary packages: 
    
    \mint{bash}|conda env create -f environment.yml|

    \item Fix errors if unmerged into master OpenAI Gym branch
    
    \begin{itemize}
        \item Consistent with the pull request at \url{https://github.com/openai/gym/pull/930}, in each of the game files in \path{gym/gym/envs/classic_control/}, add \texttt{dtype=np.float32} to each \texttt{spaces.Box()} initialization to suppress the logger warning
    \end{itemize}
    
    \item Run \texttt{main.py}
    
    \begin{itemize}
        \item \texttt{python main.py -h} for command-line argument help
        \item \texttt{python main.py -g CartPole-v0 -m DQN\_GS -a EpsilonGreedy -e 1000 --nreplay 10000} for training a normal grayscale DQN
        \item \texttt{python main.py -g CartPole-v0 -m NoTraining -a Random -e 1000 --nreplay 10000} for a random policy
    \end{itemize}
\end{enumerate}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
