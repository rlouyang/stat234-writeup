\documentclass[11pt]{article}
\usepackage{final_project}

\usepackage{silence}
\WarningFilter{hyphenat}{*******************************}

\usepackage{todonotes}
\usepackage{microtype}
\usepackage[htt]{hyphenat}
\usepackage{longtable}
\usepackage{pgffor}

\usepackage[utf8]{inputenc} 
% \doublespacing

% \usepackage[nomarkers]{endfloat}

\newcommand{\cp}{\texttt{CartPole}}
\newcommand{\ab}{\texttt{Acrobot}}
\newcommand{\mc}{\texttt{MountainCar}}

\title{State-Space Reduction in Deep Q-Networks}
\author{Michael Ge\thanks{\url{michaelge@college.harvard.edu}} 
        \and 
        Richard Ouyang\thanks{\url{rouyang@college.harvard.edu}}}
\date{\small{\url{https://github.com/hahakumquat/stat234-project}}\\
      April 24, 2018}

\begin{document}

\maketitle

\begin{abstract}
Deep convolutional neural networks have become a popular approach to estimating Q-value functions in reinforcement learning problems. These deep Q-networks take in entire images as network inputs, often resulting in large state spaces and long learning times. In this paper, we explore the use of principal component analysis to reduce the state space of network inputs for small network sizes. After testing multiple network configurations, we determine that a reduction in uninformative state features through PCA helps improve the performance of deep reinforcement learning for small neural network architectures.
\end{abstract}

\newpage
\tableofcontents

\twocolumn
\newpage

\section{Introduction}

Current methods of Q-learning with deep neural networks are inhibited by the large state space inherent in processing images. The state-of-the-art methods used in recent papers \cite{mnih2013playing, mnih2015human, van2016deep} require often infeasible amounts of computational power, time, and data. Although convolutional neural networks theoretically reduce the learning time by restricting the size of the network and accounting for structural information in the data, it is still difficult to learn a good policy in a time- and data-efficient manner. We experiment with reducing the state space dimensionality required by deep Q-networks (DQNs) by applying principal component analysis (PCA) to improve learned policies and training times on a variety of O\-pen\-AI games. Although state-space reduction has previously been studied in reinforcement learning \cite{kishima2013reduction}, use of PCA in reducing state spaces, particularly for neural networks, has not previously been considered. 

In addition, we provide an extensible and easy-to-use software framework to test various types of agents and Q-networks, even for games not tested in this paper (such as Atari games).

\section{Background}

\subsection{MDP Overview}

We will briefly describe the general Markov Decision Process (MDP)
framework. Define an MDP to be a tuple consisting of the following elements:

\begin{itemize}
    \item $\mathcal{S}$, the set of states. In our usage, each state $s \in \mathcal{S}$ is a transformation of the game screens, which are matrices of pixel values.
      
    \item $\mathcal{A}$, the set of actions. The available actions $a \in \mathcal{A}$
    depend on the setting of the game. In the problems of interest, we
    consider games with discrete, relatively simple action spaces.
      
    \item $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, the reward
    function. A reward $r_t(s_t, a_t)$ is given to the agent after an
    action $a_t$ is taken at state $s_t$. The reward is often a complex
    function. If we knew the reward function, we would easily know which
    action to take at any state $s_t$ and timestep $t$.
      
    \item $p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to
    \mathbb{R}$, the transition probabilities. After taking action $a$, the agent moves from state $s_t$ to state $s_{t+1}$ with probability $p(s_{t+1} |s_t,
    a_t)$. In our games, transitions are deterministic, so probabilities
    are either $0$ or $1$.
      
    \item $\gamma \in [0, 1]$, the discount factor. $\gamma$ discounts
    future rewards at a constant, compounded rate.
    
    In reinforcement learning, we seek to learn a policy
    $\pi: \mathcal{S} \to \mathcal{A}$ that maximizes the expected
    discounted sum of future rewards at a given state:
    $$R_t = \mathbb{E}\left[\sum_{i=t}^\infty \gamma^{i-t} r_{i}|s_{t-1}\right]$$
    
    We define the Q-function under a policy $\pi$ as the expected
    discounted sum of future rewards given a state and desired action:
    $$Q^\pi(s_t, a_t) = \mathbb{E}\left[R_t | s_{t-1}, a_{t-1} \right]$$
    
    The optimal Q-function has been proven to satisfy the Bellman equation:
    $$Q^\star(s, a) = r(s, a) + \gamma \max_{a' \in \mathcal{A}} Q^\star(s', a')$$
\end{itemize}


\subsection{Deep Q-Networks}

We now briefly discuss the application of deep neural networks to reinforcement learning. The most common use of deep neural networks is to model the Q-function. These deep Q-networks (DQNs) take in a transformed image input $s$, passes it through multiple layers, and returns a vector consisting of the estimated $Q(s, a)$ for all available actions $a \in \mathcal{A}$. 

Current state-of-the-art models \cite{mnih2013playing, mnih2015human} use deep convolutional neural networks (CNNs) as models for the Q-function; these DQNs capitalize on the structure of image data -- for example, correlations between nearby pixels -- to make Q estimates. Unfortunately, DQNs typically require a large number of parameters, thus consuming large amounts of computational resources, in terms of both time and space. In our application, due to computational constraints, we restrict our network architectures to contain about 7000 parameters and briefly explore larger parameter spaces toward the end of the paper. More information about the structure of CNNs is available in Figure \ref{fig:cnn}.

Double DQNs (DDQNs) \cite{van2016deep} are very similar to regular DQNs. However, instead of using the same network to obtain the target value for minibatch updates as in a regular DQN, the DDQN randomly selects one of two networks to update and uses the other network to obtain the target value.

\begin{figure*}
    \centering
    \includegraphics[trim={0 1cm 0 1cm}, clip, width=0.95\textwidth]{cnn.pdf}
    \caption{A visualization of the network structure of a convolutional neural network. The input is the image, which is convolved through several layers, each of which contains several filters. The result is then shrunk through a max pooling layer and passed through a fully connected linear layer before being output as Q-values for each action in the action space.}
    \label{fig:cnn}
\end{figure*}

\subsection{Principal Component Analysis}

Principal component analysis (PCA) is a dimensionality reduction technique that orthogonally projects the original features into a smaller set of features. PCA chooses the axes with the most variance to construct these principal components. Each of the resulting principal components is uncorrelated with and orthogonal to all the other components. In interpreting images, PCA keeps transformations of the most variable pixels, eliminating the information contained in pixels with very little variance.


We briefly describe how PCA works. Consider a setting where we have $n$ state feature vectors $x \in \mathbb{R}^m$. We construct a feature covariance matrix $\mathbf{S}$ as the following:

$$\sum_{i=1}^n x_i x_i^\top = \mathbf{X}^\top \mathbf{X} = \mathbf{S}$$

We then consider the $d$ largest eigenvalues of $\mathbf{S}$. It has been shown that the corresponding eigenvectors of these eigenvalues encompass the maximum amount of variance in the dataset in the subspace $\mathbb{R}^d$. We then construct a $d \times m$ projection matrix $\mathbf{U}$:

$$\mathbf{U} = (\mathbf{u}_1, \ldots, \mathbf{u}_d)^\top$$

For each $\mathbf{x}$, we can then compute the transformed feature vector $\mathbf{z} = \mathbf{U}\mathbf{x}$. These can also be interpreted as the reconstruction coefficients.

To reconstruct a simplified version of the original feature vector, we compute:

$$\hat{\mathbf{x}} = \mathbf{U}^\top \mathbf{z}$$

In our usage, we transform our input images into a feature vector of pixels before performing PCA.


\subsection{OpenAI Gym}

OpenAI Gym \cite{brockman2016openai} offers a set of many standard games on which to test reinforcement learning algorithms. We tested our algorithms, which learn on pixel images, on the following games. See Figure \ref{fig:game_images} for sample images of each game.

\begin{description}
    \item[\href{https://gym.openai.com/envs/CartPole-v0/}{CartPole-v0}] \cp~is the classic reinforcement learning environment. At every time step, the agent receives a reward of $1$ and chooses to push the cart either right or left. The game ends when the pole falls to an angle greater than 15 degrees from vertical or when the cart moves off the screen. 
    \item[\href{https://gym.openai.com/envs/Acrobot-v1/}{Acrobot-v1}] \ab~is a reinforcement learning game consisting of a double pendulum. The agent can only apply force to the connecting joint (left, right, or no movement) and must swing the pendulum so that the tip of the pendulum reaches a certain height. The agent receives a reward of $-1$ for every time step the goal is not reached.
    \item[\href{https://gym.openai.com/envs/MountainCar-v0/}{MountainCar-v0}] \mc~is a reinforcement learning game in which the agent tries to drive a car up a hill. The car is not strong enough to climb the hill itself and thus must learn to use gravity to achieve the necessary speed. At every time step, the three movements available are accelerating to the right, accelerating to the left, and doing nothing. The agent receives a reward of $-1$ for every time step the goal is not reached. 
\end{description}

\begin{figure*}[!ht]
\centering
\foreach \imagetype in {original, processed}
{
    \subfloat[Sample \imagetype~image of the \cp~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/CartPole_\imagetype.pdf}
        \label{subfig:CartPole_\imagetype}
    }
    \hfill
    \subfloat[Sample \imagetype~image of the \ab~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/Acrobot_\imagetype.pdf}
        \label{subfig:Acrobot_\imagetype}
    }
    \hfill
    \subfloat[Sample \imagetype~image of the \mc~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/MountainCar_\imagetype.pdf}
        \label{subfig:MountainCar_\imagetype}
    }
}
\caption{Original and processed images from the three games discussed in this paper. The processed images were created by grayscaling the original images and resizing from $600 \times 400$ pixels to $80 \times 80$ pixels. The resulting feature space is then $\mathbb{R}^{6400}$.}
\label{fig:game_images}
\end{figure*}

\subsection{PyTorch}

PyTorch \cite{paszke2017pytorch} is a Python framework for constructing neural network architectures. Although PyTorch is still in its early stages of development, it has several advantages compared to other neural network libraries such as TensorFlow and Keras. Namely, improvements include a clean extension of the common Python package \texttt{NumPy}, deep integration with Python, and efficient memory usage. 

\section{Methods}

\subsection{Initial Setup}

We replicated the original deep Q-network papers \cite{mnih2013playing, mnih2015human} in Python, using PyTorch \cite{paszke2017pytorch} for the neural network architecture and OpenAI Gym \cite{brockman2016openai} to test and compare our algorithms on different games. 

There are quite a few steps to get started to replicate our procedure, including changing some of the source code in OpenAI Gym. For the sake of brevity, these instructions are included in Appendix \ref{app:getting_started}. To run our experiments, we used a limited partition of an Odyssey research computing cluster to run a portion of our computationally intensive jobs.

\subsection{Image Preprocessing}

In this project, our goal was to learn the optimal policy for the games without using parametric data -- for example, the pole angle in \cp. Instead, we focused more on image processing rather than using the state information given by the Gym environment.

At each time step, we obtained the RGB (color) mapping of the game screen, resized the image to $80 \times 80$ pixels (rectangular images were squashed into a square), and converted the image to grayscale. We then took the difference between the current timestep's image and the last timestep's image as the state to input into our DQN. 

\subsection{Model Structures}
\label{subsec:model_structure}

In our experiments, we used four different types of models, each of which includes the original and double network variants: 

\begin{description}
    \item[(D)DQCNN] The original model \cite{mnih2013playing, mnih2015human} and variants \cite{van2016deep}. These models convolve images using no extra state-space reduction techniques like PCA. This model is also referred to as (D)DQN-GS, since this is the original grayscale model. We use these two terms interchangeably.
    
    \item[(D)DQN-PCA] These models use a database of 1000 states, generated from a random policy, as training data for PCA and project the pixel features of new states onto the subspace that captures at least 99\% of the variance. The results is a one-dimensional vector with varying lengths based on the game (100 to 500 features). This vector is passed into a feed-forward (not convolutional) deep neural network with varying numbers of layers and nodes to predict the Q-value.
    
    \item[(D)DQCNN-PCA] These models also perform PCA; however, the (D)DQCNN-PCA models then invert the PCA transformation, converting each transformed image back to its original space. The result is a simplified $80 \times 80$ image, which is then input into a model with the same network structure as the original (D)DQN.
    
    \item[(D)DQCNN-PCA-Mini] Out of curiosity, we included one additional model type, which takes the one-dimensional vector obtained from the PCA transformation and reshapes it into a square (zero padding if necessary). We then convolved the resulting ``image'' through a smaller neural network to predict the Q-value for each state. Although PCA strips correlation between nearby pixels, we were interested in investigating whether the reduced dimensionality preserved some structure between nearby values.
\end{description}

For the (D)DQCNN and (D)DQCNN-PCA models, we kept our network structure similar but reduced in parameter size to \cite{mnih2013playing} and \cite{mnih2015human} by using three hidden layers in each network. These DQNs take in an $80 \times 80$ grayscale image. Each hidden layer in the network convolves the previous layer's output into multiple filters, followed by a batch normalization and a leaky ReLU with $\alpha = 0.001$ to prevent dead nodes. The first layer convolves $8$ $8 \times 8$ filters with stride $4$; the second layer convolves $16$ $4 \times 4$ filters with stride $2$; the final hidden layer convolves $16$ $4 \times 4$ filters, this time with stride $1$. A max pooling with kernel size $2$ is then applied to reduce the dimensionality of the output. The final layer is fully connected and maps linearly to the number of actions available. For instance, since \cp~contains two actions at any time, the output of the neural network contains two nodes. This structure saves time when forwarding through the network, since a single forward pass for a given state $s$ gives $\hat{Q}_\text{network}(s, a)$ for all $a \in \mathcal{A}$. The images can be found in \ref{fig:pca_images}.

To ensure that the models are relatively comparable, we kept the number of parameters for each model as close as possible at around 7,000 parameters. For the (D)DQN-PCA model, we tested a variety of network architectures, varying the number of layers as well as the number of nodes in each layer. 

For the (D)DQCNN-PCA-Mini model, we kept the rough network structure the same as our original models. However, to account for the smaller image size, we reduced the kernel size to 4 and the stride to 1 for all layers.

\begin{figure*}[!ht]
\centering
\foreach \pca in {DQCNN-PCA, DQCNN-PCA-Mini}
{
    \subfloat[Sample PCA-transformed image of the \cp~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/CartPole_\pca.pdf}
        \label{subfig:cartpole_\pca_image}
    }
    \hfill
    \subfloat[Sample PCA-transformed image of the \ab~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/Acrobot_\pca.pdf}
        \label{subfig:acrobot_\pca_image}
    }
    \hfill
    \subfloat[Sample PCA-transformed image of the \mc~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/MountainCar_\pca.pdf}
        \label{subfig:mountaincar_\pca_image}
    }
}
\caption{PCA images from the three games discussed in this paper. The PCA images were created by grayscaling the original images, using the features that capture 99\% of the variance, and either inverting the transformation or not. The resulting feature space depends on the PCA reduction amount. Note that the most important pixels (near the game object in the first row, and near the top in the second row) are much more variable in color than the less important pixels, which tend to be a relatively uniform gray.}
\label{fig:pca_images}
\end{figure*}

\subsection{Hyperparameters}

We tested many configurations of hyperparameters, varying the following: 

\begin{description}
    \item[Model] One of the Q-function models mentioned in Section \ref{subsec:model_structure}.
    
    \item[Frame skip] The number of time steps for which the same action is repeated. To speed up learning, we used this method described in \cite{mnih2013playing, mnih2015human}. When an action is selected, that action is repeated for the next $k$ frames, since selecting an action and training takes much more time than rendering an additional step of the game. We chose $k = 3$ since $k = 4$, which was suggested in \cite{mnih2013playing, mnih2015human}, empirically performed worse in \cp. 
    
    \item[Update frequency] The number of distinct action selections between each minibatch update. This value is typically set to $4$. This technique speeds up the training even further, since rendering the environment is far less costly than forwarding through the network. This has been used in other papers \cite{mnih2013playing, mnih2015human, van2016deep}.
    
    \item[Number of training steps] The length of time the model should be trained, expressed in terms of the number of minibatch training steps. While state-of-the-art research uses up to 10 million frames, our resources restrict us to values between 10 thousand and 100 thousand.
    
    \item[Replay memory size] The maximum number of transitions -- each of which is a $(s, a, r, s')$ tuple -- stored in the replay memory. The replay memory is used to reduce correlation between transitions used in each minibatch update \cite{mnih2013playing, mnih2015human}.
    
    \item[Target update] If the model uses a target network \cite{mnih2015human}, this represents the number of trains between each target network update. Each update involves setting the target network parameters to the main network parameters.
    
    \item[Learning rate] The (initial) learning rate used by the optimizer. A higher learning rate results in a model that learns faster but may diverge.
    
    \item[Learning rate annealing] Whether the learning rate is annealed to a smaller value. Learning rate annealing helps decrease the likelihood of policy divergence. The learning rate anneals according to the equation $$\alpha_t = \alpha_\text{initial} \max\left(\exp\left(-\frac{t}{\lambda_\alpha}\right), \frac{0.0005}{\alpha_\text{initial}}\right),$$ where $t$ is the number of minibatch training updates that have already occurred, and $\lambda$ is the decay rate, so that the learning rate anneals exponentially from its initial value to a final value of $0.0005$.
    
    \item[Batch size] The minibatch size, or the number of transitions used in each training update. The two values tested were $32$ and $128$.
    
    \item[Loss function] The loss function used in the model. The Huber loss (defined as the $\ell_2$ norm for losses greater than 1 and the $\ell_1$ norm otherwise) and the mean squared error (MSE) loss were the two loss functions considered.
    
    \item[Regularization] The weight decay in the optimizer. This value typically ranges from $0$ to $1$ and is used to prevent overfitting in our networks.
    
    \item[Agent action selection] We used an $\epsilon$-greedy agent, annealing $\epsilon$ from $\epsilon_\text{initial} = 1$ to $\epsilon_\text{final} = 0.1$ according to the equation $$\epsilon_t = \epsilon_\text{final} + (\epsilon_\text{initial} - \epsilon_\text{final}) \exp\left(-\frac{t}{\lambda_\epsilon}\right),$$ where $t$ is the number of times the agent has already selected an action, and $\lambda$ is the decay rate. 
    
    \item[Network Architecture] Toward the end of our experimentation, we had the time to test different network configurations for the (D)DQN-PCA model. Unfortunately, due to the computational demands of performing over images, it was intractable to test larger node structures.
\end{description}

\subsection{Implementation Details}

The code is structured such that the user can pass a model, agent, game, and a large selection of hyperparameters into \texttt{main.py}. During the script execution, statistics are logged for later analysis. 

Within our \path{utils/} folder, we implemented several scripts to aid in the data analysis process, some of which we will briefly discuss here. 

\begin{itemize}
    \item \texttt{Logger.py} is a general logging utility for storing statistics in files. 
    \item \texttt{PCA.py} contains a PCA class whose objects can train on inputs of states, return the transformed features, and invert already-transformed features back into the original image space. 
    \item \texttt{ReplayMemory.py} contains a class whose objects store and quickly sample transitions for minibatch updates. 
    \item \path{get_notes_and_stats.py} parses our raw data into an easy-to-read format, providing summary statistics, hyperparameters, and metadata about each job. 
    \item \path{plot_data.py} plots raw data as well as running means over episodes and training updates. 
    \item \path{save_states.py} runs a random policy for each game, saving a large set of states on which PCA can later be performed. 
\end{itemize}
 
Our \path{automate_run} folder contains the following bash scripts for running jobs on the computing cluster:

\begin{itemize}
    \item \path{gen_slurms.sh} and \path{search_nets.sh} iterate over a given hyperparameter space and submit batch jobs for each hyperparameter combination in parallel.
    \item \path{view_plots.sh} is a utility that allows the user to view the rewards of each game, providing a preliminary categorization of model performance. 
\end{itemize}

Although we did have access to a computing cluster, the access was limited and we did not have access to GPUs, which significantly speed up learning for neural networks. We also had limited time to run our experiments, preventing us from running the 10 million frames per game prescribed in the literature \cite{mnih2013playing, mnih2015human}. This was the motivation for our efforts to improve learning by reducing the state space.

\section{Results}

The full set of raw data, metadata, and plots are available at this \href{https://github.com/hahakumquat/stat234-project/tree/master/data}{link} (warning: approximately 600 MB). For all our experiments, our primary measure of policy performance was the mean reward over the second half of the episodes, with a rolling mean over the previous 5\% of the total number of episodes providing a similar but smoother representation of model performance.

All summary statistics are taken over the second half of the experiment, after the algorithm has had time to learn the game and converge. This prevents the learning process from affecting measurements of the final learned policy.

Note that these plots are graphed across all of the episodes completed within a constant number of training iterations. Therefore, note that the number of episodes varies depending on the durations of each game. That is, for longer game durations, we have fewer episodes. For the sake of comparison within games, the $y$-axes for the reward graphs are standardized, even if episode rewards are outside the axis limits. In particular, the $y$-axis is clipped to be between 0 and 200 for \cp, $-2000$ and 0 for \ab, and $-5000$ and 0 for \mc. We do this because unclipped axes often create difficulties in detecting patterns in regions with low reward variance. 

One notable yet intentional quirk of our data collection scheme is the fact that each of our trials contains a different number of episodes, even when considering models within the same game. This difference occurs because we seek to keep training time constant between models. For instance, if we were to train models for a constant number of episodes, a poor \mc~model that averages episode durations around 5000 (and thus per-episode rewards around -5000) would be able to use approximately ten times more training time (measured in clock time) than a better \mc~model that averages episode durations around 500 (and thus per-episode rewards around -500). On the other hand, by training models for a constant number of training steps, we ensure that training time as well as the number of minibatch updates is relatively constant between all models. This strategy is consistent with the deep Q-learning literature \cite{mnih2013playing, mnih2015human, van2016deep}, which typically keeps the number of frames played constant.

\subsection{Random Policy}

As a baseline, we ran a random policy on each game, with results in Figures \ref{fig:CartPole_random}, \ref{fig:Acrobot_random}, and \ref{fig:MountainCar_random}. The average rewards and durations for the three games are available in Table \ref{tab:random_rewards}. With this initial goal in mind, we proceeded to test different model architectures and hyperparameter settings to achieve good empirical results over all three games.

\foreach \game in {CartPole, Acrobot, MountainCar}
{
    \begin{figure*}[!ht]
        \centering
        \subfloat[Rewards by episode, as well as a rolling mean, for a random policy in the \texttt{\game}~game. ]{%
            \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_rewards.pdf}
            \label{subfig:\game_NoTraining_Random_rewards}
        }
        \hfill
        \subfloat[Durations by episode, as well as a rolling mean, for a random policy in the \texttt{\game}~game.]{%
            \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_durations.pdf}
            \label{subfig:\game_NoTraining_Random_duraions}
        }
    \caption{Baseline results from the \texttt{\game}~game from OpenAI Gym under a random policy.}
    \label{fig:\game_random}
    \end{figure*}
}

\begin{table*}[!ht]
    \footnotesize
    \centering
    \begin{tabular}{c|cc}
        \toprule
        Game & Average Duration & Average Reward \\ \midrule
        CartPole-v0 & 18.441 & 18.441 \\
        Acrobot-v1 & 599.73 & -598.73 \\
        MountainCar-v0 & 2717.77 & -2717.77 \\
        \bottomrule
    \end{tabular}
    \caption{Average rewards and durations under a random policy for the three games.}
    \label{tab:random_rewards}
\end{table*}

% \subsection{Initial Exploration}

% Although \cite{mnih2013playing, mnih2015human} used a learning rate of $0.00025$, we found that learning was far too slow for our purposes and instead opted for a larger learning rate. We began with a learning rate of $0.1$, but this was far too large: CartPole would learn to durations of around 200 time steps within a few dozen iterations but would quickly diverge. The other two games also did not converge. Significant improvements occurred after reducing to a learning rate of about $0.001$. 

% We used

% \begin{itemize}
%     \item \cp~learning rate 001
% \end{itemize}

\subsection{Hyperparameter Tuning Results}

In our hyperparameter tuning, we decided to keep the frame skip, update frequency, replay memory size, and agent action selection hyperparameters constant for testing, since grid search sizes increase exponentially in the number of parameters, and varying these settings would vastly increase simulation time and space consumption. 

\subsubsection{First Grid Search (10,000 Iterations)}

In order to establish a solid set of hyperparameters to which PCA variants of DQNs could be compared, we began our experiments with an initial grid search over the hyperparameters of interest for 10,000 training steps to eliminate poorly performing hyperparameter settings. Table \ref{tab:grid_search_v1_10k} contains the results of the top fifteen models for each game ordered by performance.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v1_10k.tex}
    
    \caption{Top fifteen parameter-tuned mean rewards per game for our first grid search. Each run used either the DDQN-GS or DQN-GS model and lasted for 10,000 training steps. Note that DDQNs do not have a target update option, so their value in the ``target'' column is N/A.}
    \label{tab:grid_search_v1_10k}
\end{table*}

From these results, we already find that our best hyperparameter configurations beat the random baseline. \mc~was rather difficult to learn, as the final policy seems to depend on the random initialization. With sub-optimal hyperparameters, the episodes under a poorly learned policy can extend to tens of thousands of time steps, far more than a random policy. This particular vulnerability to divergence likely occurs because all transitions entered into the replay memory have $r = -1$, creating difficulties in learning. The average episode in \cp~roughly doubles the duration of the random policy, though the performance has significantly high variance in all models, reaching maxima of up to 200 time steps. Finally, \ab~seems to have been learned quite quickly, reaching optimal performance within the 10,000 training steps.

Here, we observe consistently strong performance by DDQN-GSs over DQN-GSs and by using a Huber loss function over an MSE loss. Other optimizers, such as Adam, failed to learn in pre-testing, so we used the RMSProp optimizer (as suggested by \cite{mnih2013playing}) throughout our experiments. Weight decay yielded mixed results, likely due to the fact that we only allowed the models to run for 10,000 training steps. Additionally, the effectiveness of the learning rate parameters varied significantly with the game; for example, \cp~learns better with a significantly higher learning rate than the other two games. We also saw that annealing the learning rate slightly improved the overall performance of the algorithms, so we decided to keep the annealing for subsequent experiments. In theory, a learning rate annealing should improve the stability of the model after the model has had time to learn a strong policy. Moreover, not displayed in the data is the surprising fact that DQN-GSs with an enabled target update performed significantly worse than DQN-GSs without a target update in that they failed to reach any convergence at all, so we also decided to remove target update functionality for future experiments. With these observations in mind, we proceeded to test another batch of models using a larger number of training steps. 

\subsubsection{Second Grid Search (50,000 Iterations)}
 
Setting the optimizer, loss function, and target update to RMSProp, Huber loss, and False, respectively, we performed a second grid search over 50,000 training iterations to tune the learning rate and weight decay. The results can be found in Table \ref{tab:grid_search_v2_50k}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v2_50k.tex}
    
    \caption{Top fifteen parameter-tuned mean rewards per game for our second grid search. All experiments used a Huber loss, learning rate annealing, no target update, and the RMSProp optimizer for 50,000 training steps.}
    \label{tab:grid_search_v2_50k}
\end{table*}

\mc~and \ab~continued to perform well with more training time for the best models, though we began to see evidence of high variance in performance between models for these games. Both had the worst models getting average scores near the level of randomness. In these games, episodes are not completed until the agent succeeds at reaching above a certain height or climbing up the mountain. As a result, if an agent learns a poor policy early on, the agent could spend all of its training iterations receiving $-1$ reward with no positive feedback. To help mitigate this issue, we capped the number of training iterations per episode to 20,000 with the intent to expose the agent to more positive states and to speed up learning time. If an agent reaches a duration of 20,000, we already know that the agent does a poor job of learning a good policy and can afford to ignore the model, since in our random policy experiments, episode durations never surpassed 16,000 and rarely lasted more than 10,000.

Again, we see that DDQN-GSs tend to outperform DQN-GSs. The learning rate, however, is inconsistent from the first grid search. Here, we find that overall, lower learning rates are preferred over higher ones for longer grid search durations. This is reasonable, since we expect that with a longer training duration, the agent has more time to converge to a good policy, whereas with only 10,000 iterations, the model needs a higher learning rate to quickly outdo random performance. As a result, we decided to continue with even higher training times with lower learning rates. Weight decay does not appear to have a significant impact on the performance of the models, so we decided to take the more conservative option of a regularization coefficient of $0.1$.

\subsubsection{Third Grid Search (100,000 Iterations)}

By this point, we established that DDQNs tended to outperform DQNs, so we then explored hyperparameter configurations that yielded good performance over 100,000 iterations. We then performed a grid search, varying the batch size and learning rate. The results can be found in \ref{tab:grid_search_v3_100k}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v3_100k.tex}
    
    \caption{Complete results for our third grid search, sorted by mean reward. All experiments used the DDQN-GS model with a Huber loss function, learning rate annealing, and a $0.1$ weight decay for 100,000 training iterations.}
    \label{tab:grid_search_v3_100k}
\end{table*}

The data here indicate mixed results. \mc~tends to do better with larger batch sizes, while \ab~and \cp~do better with a smaller batch size. The learning rates differed from game to game as well. With no significant trends in performance, we opted to use a batch size of 128 to increase learning speed and a learning rate of $0.001$ to prevent divergence. Our final hyperparameters, used in the rest of the paper, are available in Table \ref{tab:grid_search_hyperparameters}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \begin{tabular}{c|c}
        \toprule
        Hyperparameter & Value \\ \midrule
        Model & DDQN \\
        Frame skip & $3$ \\
        Update frequency & $4$ \\
        Training updates & $100000$ \\
        Replay memory size & $10000$ \\
        Target update & N/A \\
        Learning rate & $0.001$ \\
        Learning rate annealing & Yes \\
        Batch size & $128$ \\
        Loss function & Huber loss \\
        Regularization & $0.1$ \\
        \bottomrule
    \end{tabular}
    \caption{The hyperparameters found by our grid search and used for the PCA variants of the (D)DQN.}
    \label{tab:grid_search_hyperparameters}
\end{table*}

\subsection{PCA Networks}

The hyperparameter comparison results generalized quite well to other models, so we continued to use these parameters for . See Figures \ref{fig:CartPole_final_v1_100k}, \ref{fig:Acrobot_final_v1_100k}, and \ref{fig:MountainCar_final_v1_100k} for a graphical comparison of the PCA networks tested. Table \ref{tab:final_v1_100k} contains the full set of results. 

\foreach \game in {CartPole, Acrobot, MountainCar}
{   
    \begin{figure*}[!ht]
        \centering
        
        \subfloat[Rewards by episode from training the DDQN-GS model on the \texttt{\game} game. \label{subfig:\game_DDQN-GS_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQN-GS_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQN-PCA model on the \texttt{\game} game. \label{subfig:\game_DDQN-PCA_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQN-PCA_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQCNN-PCA model on the \texttt{\game} game. \label{subfig:\game_DDQCNN-PCA_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQCNN-PCA_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQCNN-PCA-Mini model on the \texttt{\game} game. \label{subfig:\game_DDQCNN-PCA-Mini_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQCNN-PCA-Mini_EpsilonGreedy_rewards_cpu.pdf}
        }
            
        % \foreach \model in {DDQN-GS, DDQN-PCA, DDQCNN-PCA, DDQCNN-PCA-Mini}
        % {
        %     \subfloat[Rewards by episode from training the \model~model on the \texttt{\game}~game. \label{subfig:\game_\model_EpsilonGreedy_rewards_cpu}]{%
        %         \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_rewards_cpu.pdf}
        %     }
        %     % \hfill
        % }
        % \caption{}
        \caption{Rewards by episode for four DQN variants (including those using PCA) on the \texttt{\game} game. All trials used the hyperparameters detailed in Figure~\ref{tab:grid_search_hyperparameters}.}
        \label{fig:\game_final_v1_100k}
    \end{figure*}
}

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/final_v1_100k.tex}
    
    \caption{Model comparison with final hyperparameters over 100,000 training iterations.}
    \label{tab:final_v1_100k}
\end{table*}

First, we note that the (D)DQN-PCA model, which does not convolve over an image representation, does relatively poorly in all settings. This is not surprising, since this model does not account for the structural properties of adjacent pixels as a convolutional neural network does, the model loses a significant amount of information about the image. It was possible that the neural network architecture selected for these models were underfitting the data, so we later decided to consider increasing the network complexity. For now, we turn our attention to the convolutional PCA versions of the models.

The DDQN-PCA model performed the best for all three games, outperforming even the convolutional models and generating some of the best scores and most stable means we ever saw in our experiments. One possible reason for the superior performance is the fact that PCA simplifies the images, thereby reducing noise and allowing for more efficient learning. Unfortunately, these results were not replicated in the \mc~game, failing to beat even the random baseline. It seems that \mc~is sensitive to weight initialization, causing unpredictable fluctuations in its performance.

Despite some PCA variants being high performers in each of the games, many of the remaining PCA models were unable to perform above a random baseline, particularly those not using convolutional neural networks. Upon closer inspection of the reward graphs, we find that several PCA models reached a strong optimum around the middle of the training process, but ended up diverging over time. Overall, we find that PCA is sensitive to divergence for bad parameter settings, but less so than the original model; additionally, under the right use cases, the right learning rate annealing, and $\epsilon$ decay, PCA is a promising method to increase the performance of a Q-network under short learning time settings.

\subsubsection{Testing Layer Sizes}

With a strong DDQN-GS model selected as our new baseline, we then began to test the performance of PCA-based models. With \mc~and \ab~being solved problems for several of our neural networks but \cp~failing to ever reach particularly large scores, we then focused our efforts on improving \cp. Due to limited time and resources, we had to test various models, we left tuning neural network architectures as our final step, taking the discovered hyperparameters up to this point for granted. We then tested the (D)DQN-PCA on a variety of more complex neural network designs. See Table \ref{tab:model_selection_DDQN_PCA_6} for the results from testing several feed-forward deep neural network architectures for the DDQN-PCA model over 20,000 training iterations.

These findings show a significant increase in the performance of both models, despite losing the structural correlations between pixels. It seems that our initial neural network architecture was ultimately underfitting the images, yielding results limited to the mid-60's for \cp. By adding more weights, (initially 7,000 parameters but now over 20,000), we were able to see strong performances from these previously underperforming models.

With these results, we decided to use a two-layer neural network architecture, containing 128 nodes on the first and 64 nodes on the second.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/model_selection_DDQN_PCA_6.tex}
    
    \caption{Results from testing several feed-forward deep neural network architectures for the DDQN-PCA model, sorted by reward in descending order. Only the top fifteen results from each game are shown. All experiments used a Huber loss function, learning rate annealing, a batch size of $128$, a learning rate of $0.001$, and a $0.1$ weight decay for 20,000 training iterations.}
    \label{tab:model_selection_DDQN_PCA_6}
\end{table*}

\subsubsection{Further Testing}

With our new DDQN-PCA model, we 

\foreach \game in {CartPole, Acrobot, MountainCar}
{   
    \begin{figure*}[!ht]
        \centering
        
        \subfloat[Rewards by episode from training the DDQN-GS model on the \texttt{\game} game. \label{subfig:\game_DDQN-GS_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQN-GS_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQN-PCA model on the \texttt{\game} game. \label{subfig:\game_DDQN-PCA_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQN-PCA_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQCNN-PCA model on the \texttt{\game} game. \label{subfig:\game_DDQCNN-PCA_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQCNN-PCA_EpsilonGreedy_rewards_cpu.pdf}
        }
        \hfill
        \subfloat[Rewards by episode from training the DDQCNN-PCA-Mini model on the \texttt{\game} game. \label{subfig:\game_DDQCNN-PCA-Mini_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_DDQCNN-PCA-Mini_EpsilonGreedy_rewards_cpu.pdf}
        }
            
        % \foreach \model in {DDQN-GS, DDQN-PCA, DDQCNN-PCA, DDQCNN-PCA-Mini}
        % {
        %     \subfloat[Rewards by episode from training the \model~model on the \texttt{\game}~game. \label{subfig:\game_\model_EpsilonGreedy_rewards_cpu}]{%
        %         \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_rewards_cpu.pdf}
        %     }
        %     % \hfill
        % }
        % \caption{}
        \caption{Rewards by episode for four DQN variants (including those using PCA) on the \texttt{\game} game. All trials used the hyperparameters detailed in Figure~\ref{tab:grid_search_hyperparameters}.}
        \label{fig:\game_final_v2_100k}
    \end{figure*}
}

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/final_v2_100k.tex}
    
    \caption{Model comparison with final hyperparameters over 100,000 training iterations.}
    \label{tab:final_v2_100k}
\end{table*}

\section{Discussion}

Throughout this exploratory project, we found that learning to play games from image data alone makes each problem significantly harder. While we had the option to read in metadata about each state such as the pole's angle, the cart's location, or the car's velocity, we decided that the best way to standardize the performance of our model was to ensure that each game fed in only pixels. Note that there are many simulations online that claim to learn better scores for \cp~even just by using pixel data, but these simulations take advantage of knowing the cart locationcropping the whitespace around the cart, drastically improving the performance of the convolution. On the other hand, we sought to learn the games without any prior knowledge whatsoever.

One thing not discussed in great detail was how we decided upon which parameters we chose at every step. In addition to empirical evaluations, we also built generalized linear models on our hyperparameters, treating them as categorical variables and predicting the mean. By examining the coefficients of the model, this gave us a rough estimate of how different features were correlated with the mean reward of each model. The results were consistent with our own observations of the performances of specific hyperparameters.

An interesting problem that we encountered while testing different models was that there was little correlation between the loss function and the performance of the model. Indeed, more often than not, the loss function hovered around the same value, typically less than $0.1$, as the simulations ran without ever converging toward an actual minimization. Typically, this indicates that we are underfitting our model, since the model is never able to minimize the loss; however, online discussions claim that this is not a problem in a reinforcement learning setting \cite{stackexchange2017loss}.

To see a less noisy version of the per-episode reward, we also kept track of changes in Q-values as the model trained. We randomly sampled a set of 128 states from the replay memory and determined the average Q-value over all states after each episode. Over time, we expect the sample states' Q-values to increase as the policy improves, and the expected future return increases. Indeed, the Q-values for the sample states do tend to increase as the policy learns. Notably, for games with negative rewards (\ab~and \mc), the sample Q-value starts around 0 and decreases sharply at the very beginning of the training before increasing slightly. This is expected and is an artifact of the fact that the model initially does not yet know that typical episodes have negative rewards. 

\section{Conclusion}

To conclude, we have built up a robust coding framework to test different Q-function models on a variety of discrete-space OpenAI games. The results suggest that compared to the original model, the de-noising and state-space reduction provided by PCA helps PCA-based variants of DQNs perform quite well in several games. However, these variants do not do as well in games with exceptionally long durations and exceptionally sparse rewards. With the right model regularization and network architecture, PCA is a promising way to increase model performance for reinforcement learning by reducing the state space and eliminating the need for DQNs to learn unimportant feature information.

\subsection{Further Directions}

Further directions include performing the same PCA analysis for DQN performance on other games, particularly Atari games. Games with large (but discrete) action spaces are also good candidates, since the games used in our experiments had only two or three actions. Additionally, extensions to games with continuous action spaces, which would likely need a significantly different network architecture (such as a convolutional neural network linked with a network in which actions are the inputs), pose an interesting problem for DQNs, whose network structure relies heavily on the presence of a discrete action space.

We had severely limited computational power and time, so extending this work using more varied learning rates and annealing schedules could offer better performance. Also, computing using GPUs (which were unfortunately not available) rather than CPUs would have been a substantial time boost.

Recent developments and additions to DQNs also suggest promising improvements to the general DQN framework as well as the use of PCA in reinforcement learning. For instance, adding hindsight experience replay \cite{andrychowicz2017hindsight} or prioritized experience replay \cite{schaul2015prioritized}, particularly to \mc, which is heavily dependent on choosing the right transitions in each minibatch update, would further improve our algorithms' performance and check the robustness of using PCA with slightly different learning algorithms.

\newpage
\bibliography{final_project}{}
\bibliographystyle{plain}

\onecolumn

\appendix

\section{Setup Instructions}
\label{app:getting_started}

\begin{enumerate}
    \item Download/install Anaconda
    
    \item Clone this paper's repository: \url{https://github.com/hahakumquat/stat234-project}
    
    \item Inside the directory containing the repository, create and activate the \texttt{conda} environment, which consists of all the necessary packages: 
    
    \begin{minted}{bash}
    conda env create -f environment.yml
    source activate stat234
    \end{minted}

    \item Fix errors if unmerged into master OpenAI Gym branch
    
    \begin{itemize}
        \item Consistent with the pull request at \url{https://github.com/openai/gym/pull/972}, in each of the game files in \path{gym/gym/envs/classic_control/}, add \texttt{dtype=np.float32} to each \texttt{spaces.Box()} initialization to suppress the logger warning
    \end{itemize}
    
    \item Run \texttt{main.py}
    
    \begin{itemize}
        \item \texttt{python main.py -h} for command-line argument instructions and a full list of arguments and defaults.
        \item \texttt{python main.py -g CartPole-v0 -m DQN\_GS -e 1000} for training a normal grayscale DQN on \cp~ for 1000 training steps
        \item \texttt{python main.py -g Acrobot-v1 -m DDQCNN\_PCA} for training a convolutional PCA variant of a DDQN
        \item \texttt{python main.py -g CartPole-v0 -a Random -e 1000} for a random policy
    \end{itemize}
\end{enumerate}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: