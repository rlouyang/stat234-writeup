\documentclass[11pt]{article}
\usepackage{final_project}

\usepackage{silence}
\WarningFilter{hyphenat}{*******************************}

\usepackage{todonotes}
\usepackage{microtype}
\usepackage[htt]{hyphenat}
\usepackage{longtable}
\usepackage{pgffor}

\usepackage[utf8]{inputenc} 
% \doublespacing



% \usepackage[nomarkers]{endfloat}

\newcommand{\cp}{\texttt{CartPole}}
\newcommand{\ab}{\texttt{Acrobot}}
\newcommand{\mc}{\texttt{MountainCar}}

\title{State-Space Reduction in Deep Q-Networks}
\author{Michael Ge\thanks{\url{michaelge@college.harvard.edu}} 
        \and 
        Richard Ouyang\thanks{\url{rouyang@college.harvard.edu}}}
\date{April 24, 2018 \\
      \small{\url{https://github.com/hahakumquat/stat234-project}}}

\begin{document}

\maketitle

\listoftodos

\begin{abstract}
Deep convolutional neural networks have become a popular approach to estimating Q-value functions in reinforcement learning problems. These deep Q-networks take in entire images as network inputs, often resulting in large state spaces and long learning times. In this paper, we explore the use of principal component analysis to reduce the state space of network inputs. After testing multiple network configurations, we determine that a reduction in uninformative state features through PCA helps improve the performance of deep reinforcement learning.
\end{abstract}
\newpage
\tableofcontents

\twocolumn
\newpage

\section{Introduction}

Current methods of Q-learning with deep neural networks are inhibited by the large state space inherent in processing images. The state-of-the-art methods used in recent papers \cite{mnih2013playing, mnih2015human, van2016deep} require often infeasible amounts of computational power, time, and data. Although convolutional neural networks theoretically reduce the learning time by restricting the size of the network and accounting for structural information in the data, it is still difficult to learn a good policy in a time- and data-efficient manner. We experiment with reducing the state space dimensionality required by deep Q-networks (DQNs) by applying principal component analysis (PCA) to improve learned policies and training times on a variety of OpenAI games. Although state-space reduction has previously been studied in reinforcement learning \cite{kishima2013reduction}, use of PCA in reducing state spaces, particularly for neural networks, has not previously been considered. 

In addition, we provide an extensible and easy-to-use software framework to test various types of agents and Q-networks, even for games not tested in this paper (such as Atari games).

\section{Background}

\subsection{MDP Overview}

We will briefly describe the general Markov Decision Process (MDP)
framework. Define an MDP to be a tuple consisting of the following elements:

\begin{itemize}
    \item $\mathcal{S}$, the set of states. In our usage, each state $s \in \mathcal{S}$ is a transformation of the game screens, which are matrices of pixel values.
      
    \item $\mathcal{A}$, the set of actions. The available actions $a \in \mathcal{A}$
    depend on the setting of the game. In the problems of interest, we
    consider games with discrete, relatively simple action spaces.
      
    \item $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, the reward
    function. A reward $r_t(s_t, a_t)$ is given to the agent after an
    action $a_t$ is taken at state $s_t$. The reward is often a complex
    function. If we knew the reward function, we would easily know which
    action to take at any state $s_t$ and timestep $t$.
      
    \item $p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to
    \mathbb{R}$, the transition probabilities. After taking action $a$, the agent moves from state $s_t$ to state $s_{t+1}$ with probability $p(s_{t+1} |s_t,
    a_t)$. In our games, transitions are deterministic, so probabilities
    are either $0$ or $1$.
      
    \item $\gamma \in [0, 1]$, the discount factor. $\gamma$ discounts
    future rewards at a constant, compounded rate.
    
    In reinforcement learning, we seek to learn a policy
    $\pi: \mathcal{S} \to \mathcal{A}$ that maximizes the expected
    discounted sum of future rewards at a given state:
    $$R_t = \mathbb{E}\left[\sum_{i=t}^\infty \gamma^{i-t} r_{i}|s_{t-1}\right]$$
    
    We define the Q-function under a policy $\pi$ as the expected
    discounted sum of future rewards given a state and desired action:
    $$Q^\pi(s_t, a_t) = \mathbb{E}\left[R_t | s_{t-1}, a_{t-1} \right]$$
    
    The optimal Q-function has been proven to satisfy the Bellman equation:
    $$Q^\star(s, a) = r(s, a) + \gamma \max_{a' \in \mathcal{A}} Q^\star(s', a')$$
\end{itemize}


\subsection{Deep Q-Networks}

We now briefly discuss the application of deep neural networks to reinforcement learning. The most common use of deep neural networks is to model the Q-function. These deep Q-networks (DQNs) take in a transformed image input $s$, passes it through multiple layers, and returns a vector consisting of the estimated $Q(s, a)$ for all available actions $a \in \mathcal{A}$. 

Current state-of-the-art models \cite{mnih2013playing, mnih2015human} use deep convolutional neural networks (CNNs) as models for the Q-function; these DQNs capitalize on the structure of image data -- for example, correlations between nearby pixels -- to make Q estimates. Unfortunately, DQNs typically require a large number of parameters, thus consuming large amounts of computational resources, in terms of both time and space.

Double DQNs (DDQNs) \cite{van2016deep} are very similar to regular DQNs. However, instead of using the same network to obtain the target value for minibatch updates as in a regular DQN, the DDQN randomly selects one of two networks to update and uses the other network to obtain the target value.

\todo{maybe math for dqns}

\subsection{Principal Component Analysis}

Principal component analysis (PCA) is a dimensionality reduction technique that orthogonally projects the original features into a smaller set of features. PCA chooses the axes with the most variance to construct these principal components. Each of the resulting principal components is uncorrelated with and orthogonal to all the other components. In interpreting images, PCA keeps transformations of the most variable pixels, eliminating the information contained in pixels with very little variance.

PCA works as follows: \todo{maybe math for pcas}

\subsection{OpenAI Gym}

OpenAI Gym \cite{brockman2016openai} offers a set of many standard games on which to test reinforcement learning algorithms. We tested our algorithms, which learn on pixel images, on the following games. See Figure \ref{fig:game_images} for sample images of each game.

\begin{description}
    \item[\href{https://gym.openai.com/envs/CartPole-v0/}{CartPole-v0}] \cp~is the classic reinforcement learning environment. At every time step, the agent receives a reward of $1$ and chooses to push the cart either right or left. The game ends when the pole falls to an angle greater than 15 degrees from vertical or when the cart moves off the screen. 
    \item[\href{https://gym.openai.com/envs/Acrobot-v1/}{Acrobot-v1}] \ab~is a reinforcement learning game consisting of a double pendulum. The agent can only apply force to the connecting joint (left, right, or no movement) and must swing the pendulum so that the tip of the pendulum reaches a certain height. The agent receives a reward of $-1$ for every time step the goal is not reached.
    \item[\href{https://gym.openai.com/envs/MountainCar-v0/}{MountainCar-v0}] \mc~is a reinforcement learning game in which the agent tries to drive a car up a hill. The car is not strong enough to climb the hill itself and thus must learn to use gravity to achieve the necessary speed. At every time step, the three movements available are accelerating to the right, accelerating to the left, and doing nothing. The agent receives a reward of $-1$ for every time step the goal is not reached. 
\end{description}

\begin{figure*}[!ht]
\centering
\foreach \imagetype in {original, processed}
{
    \subfloat[Sample \imagetype~image of the \cp~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/CartPole_\imagetype.pdf}
        \label{subfig:CartPole_\imagetype}
    }
    \hfill
    \subfloat[Sample \imagetype~image of the \ab~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/Acrobot_\imagetype.pdf}
        \label{subfig:Acrobot_\imagetype}
    }
    \hfill
    \subfloat[Sample \imagetype~image of the \mc~game screen. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/MountainCar_\imagetype.pdf}
        \label{subfig:MountainCar_\imagetype}
    }
}
\caption{Original and processed images from the three games discussed in this paper. The processed images were created by grayscaling the original images and resizing from $600 \times 400$ pixels to $80 \times 80$ pixels. The resulting feature space is then $\mathbb{R}^{6400}$.}
\label{fig:game_images}
\end{figure*}

\subsection{PyTorch}

PyTorch \cite{paszke2017pytorch} is a Python framework for constructing neural network architectures. Although PyTorch is still in its early stages of development, it has several advantages compared to other neural network libraries such as TensorFlow and Keras. Namely, improvements include a clean extension of the common Python package \texttt{NumPy}, deep integration with Python, and efficient memory usage. 

\section{Methods}

\subsection{Initial Setup}

We replicated the original deep Q-network papers \cite{mnih2013playing, mnih2015human} in Python, using PyTorch \cite{paszke2017pytorch} for the neural network architecture and OpenAI Gym \cite{brockman2016openai} to test and compare our algorithms on different games. 

There are quite a few steps to get started to replicate our procedure, including changing some of the source code in OpenAI Gym. For the sake of brevity, these instructions are included in Appendix \ref{app:getting_started}. To run our experiments, we used the Odyssey research computing cluster to run computationally intensive jobs.

\subsection{Image Preprocessing}

In this project, our goal was to learn the optimal policy for the games without using parametric data -- for example, the pole angle in \cp. Instead, we focused more on image processing rather than using the state information given by the Gym environment.

At each time step, we obtained the RGB (color) mapping of the game screen, resized the image to $80 \times 80$ pixels (rectangular images were squashed into a square), and converted the image to grayscale. We then took the difference between the current timestep's image and the last timestep's image as the state to input into our DQN. 

\subsection{Model Structures}
\label{subsec:model_structure}

In our experiments, we used four different types of models, each of which includes the original and double network variants: 

\begin{description}
    \item[(D)DQCNN] The original model \cite{mnih2013playing, mnih2015human} and variants \cite{van2016deep}. These models convolve images using no extra state-space reduction techniques like PCA. This model is also referred to as (D)DQN-GS, since this is the original grayscale model. We use these two terms interchangeably.
    
    \item[(D)DQN-PCA] These models use a database of 1000 states as training data for PCA and project the pixel features of new states onto the subspace that captures 99\% of the variance. The results is a one-dimensional vector with varying lengths based on the game (100 to 500 features). This vector is passed into a conventional (not convolutional) deep neural network to predict the Q-value.
    
    \item[(D)DQCNN-PCA] These models perform PCA as in the (D)DQN-PCA model but then invert the PCA transformation in order to convert each image back to its original space. The result is a simplified $80 \times 80$ image, which is then input into a model with the same network structure as the original (D)DQN.
    
    \item[(D)DQCNN-PCA-Mini] Out of curiosity, we included one additional model type, which takes the one-dimensional vector obtained from the PCA transformation and reshapes it into a square (zero padding if necessary). We then convolved the resulting ``image'' through a smaller neural network to predict the Q-value for each state. Although PCA strips correlation between nearby pixels, we were interested in investigating whether the reduced dimensionality preserved some structure between nearby values.
\end{description}

For the (D)DQCNN and (D)DQCNN-PCA models, we kept our network structure similar to \cite{mnih2013playing} and \cite{mnih2015human} by using three hidden layers in each network. These DQNs take in an $80 \times 80$ grayscale image. Each hidden layer in the network convolves the previous layer's output into multiple filters, followed by a batch normalization and a leaky ReLU with $\alpha = 0.001$ to prevent dead nodes. The first layer convolves $8$ $8 \times 8$ filters with stride $4$; the second layer convolves $16$ $4 \times 4$ filters with stride $2$; the final hidden layer convolves $16$ $4 \times 4$ filters, this time with stride $1$. A max pooling with kernel size $2$ is then applied to reduce the dimensionality of the output. The final layer is fully connected and maps linearly to the number of actions available. For instance, since \cp~contains two actions at any time, the output of the neural network contains two nodes. This structure saves time when forwarding through the network, since a single forward pass for a given state $s$ gives $\hat{Q}_\text{network}(s, a)$ for all $a \in \mathcal{A}$. The images can be found in \ref{fig:pca_images}.

To keep the models relatively similar in size, we kept the number of parameters for each model as close as possible. For the (D)DQN-PCA model, we tested a variety of network architectures, varying the number of layers as well as the number of nodes in each layer. 

For the (D)DQCNN-PCA-Mini model, we kept the rough network structure the same as our original models. However, to account for the smaller image size, we reduced the kernel size to 4 and the stride to 1 for all layers.

\begin{figure*}[!ht]
\centering
\foreach \pca in {DQCNN-PCA, DQCNN-PCA-Mini}
{
    \subfloat[Sample PCA-transformed image of the \cp~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/CartPole_\pca.pdf}
        \label{subfig:cartpole_\pca_image}
    }
    \hfill
    \subfloat[Sample PCA-transformed image of the \ab~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/Acrobot_\pca.pdf}
        \label{subfig:acrobot_\pca_image}
    }
    \hfill
    \subfloat[Sample PCA-transformed image of the \mc~game screen for the \pca~model. ]{%
        \includegraphics[width=0.30\textwidth]{game_images/MountainCar_\pca.pdf}
        \label{subfig:mountaincar_\pca_image}
    }
}
\caption{PCA images from the three games discussed in this paper. The PCA images were created by grayscaling the original images, using the features that capture 99\% of the variance, and either inverting the transformation or not. The resulting feature space depends on the PCA reduction amount.}
\label{fig:pca_images}
\end{figure*}

\subsection{Hyperparameters}

We tested many configurations of hyperparameters, varying the following: 

\begin{description}
    \item[Model] One of the Q-function models mentioned in Section \ref{subsec:model_structure}.
    
    \item[Frame skip] The number of time steps for which the same action is repeated. To speed up learning, we used this method described in \cite{mnih2013playing, mnih2015human}. When an action is selected, that action is repeated for the next $k$ frames, since selecting an action and training takes much more time than rendering an additional step of the game. We chose $k = 3$ since $k = 4$, which was suggested in \cite{mnih2013playing, mnih2015human}, empirically performed worse in \cp. 
    
    \item[Update frequency] The number of distinct action selections between each minibatch update. This value is typically set to $4$. This technique speeds up the training even further, since rendering the environment is far less costly than forwarding through the network. This has been used in other papers \cite{mnih2013playing, mnih2015human, van2016deep}.
    
    \item[Number of training steps] The length of time the model should be trained, expressed in terms of the number of minibatch training steps. While state-of-the-art research uses up to 10 million frames, our resources restrict us to values between 10 thousand and 100 thousand.
    
    \item[Replay memory size] The maximum number of transitions -- each of which is a $(s, a, r, s')$ tuple -- stored in the replay memory. The replay memory is used to reduce correlation between transitions used in each minibatch update \cite{mnih2013playing, mnih2015human}.
    
    \item[Target update] If the model uses a target network \cite{mnih2015human}, this represents the number of trains between each target network update. Each update involves setting the target network parameters to the main network parameters.
    
    \item[Learning rate] The (initial) learning rate used by the optimizer. A higher learning rate results in a model that learns faster but may diverge.
    
    \item[Learning rate annealing] Whether the learning rate is annealed to a smaller value. Learning rate annealing helps decrease the likelihood of policy divergence. The learning rate anneals according to the equation $$\alpha_t = \alpha_\text{initial} \max\left(\exp\left(-\frac{t}{\lambda_\alpha}\right), \frac{0.0005}{\alpha_\text{initial}}\right),$$ where $t$ is the number of minibatch training updates that have already occurred, and $\lambda$ is the decay rate, so that the learning rate anneals exponentially from its initial value to a final value of $0.0005$.
    
    \item[Batch size] The minibatch size, or the number of transitions used in each training update. The two values tested were $32$ and $128$.
    
    \item[Loss function] The loss function used in the model. The Huber loss (defined as the $\ell_2$ norm for losses greater than 1 and the $\ell_1$ norm otherwise) and the mean squared error (MSE) loss were the two loss functions considered.
    
    \item[Regularization] The weight decay in the optimizer. This value typically ranges from $0$ to $1$ and is used to prevent overfitting in our networks.
    
    \item[Agent action selection] We used an $\epsilon$-greedy agent, annealing $\epsilon$ from $\epsilon_\text{initial} = 1$ to $\epsilon_\text{final} = 0.1$ according to the equation $$\epsilon_t = \epsilon_\text{final} + (\epsilon_\text{initial} - \epsilon_\text{final}) \exp\left(-\frac{t}{\lambda_\epsilon}\right),$$ where $t$ is the number of times the agent has already selected an action, and $\lambda$ is the decay rate. 
\end{description}

\subsection{Implementation Details}

The code is structured such that the user can pass a model, agent, game, and a large selection of hyperparameters into \texttt{main.py}. During the script execution, statistics are logged for later analysis. 

Within our \path{utils/} folder, we implemented several scripts to aid in the data analysis process, some of which we will briefly discuss here. 

\begin{itemize}
    \item \texttt{Logger.py} is a general logging utility for storing statistics in files. 
    \item \texttt{PCA.py} contains a PCA class whose objects can train on inputs of states, return the transformed features, and invert already-transformed features back into the original image space. 
    \item \texttt{ReplayMemory.py} contains a class whose objects store and quickly sample transitions for minibatch updates. 
    \item \path{get_notes_and_stats.py} parses our raw data into an easy-to-read format, providing summary statistics, hyperparameters, and metadata about each job. 
    \item \path{plot_data.py} plots raw data as well as running means over episodes and training updates. 
    \item \path{save_states.py} runs a random policy for each game, saving a large set of states on which PCA can later be performed. 
\end{itemize}
 
Our \path{automate_run} folder contains the following bash scripts for running jobs on the computing cluster:

\begin{itemize}
    \item \path{gen_slurms.sh} and \path{search_nets.sh} iterate over a given hyperparameter space and submit batch jobs for each hyperparameter combination in parallel.
    \item \path{view_plots.sh} is a utility that allows the user to view the rewards of each game, providing a preliminary categorization of model performance. 
\end{itemize}

Although we did have access to a computing cluster, we did not have access to GPUs, which significantly speed up learning for neural networks. We also had limited time to run our experiments, preventing us from running the 10 million frames per game prescribed in the literature \cite{mnih2013playing, mnih2015human}. This was the motivation for our efforts to improve learning by reducing the state space.

\section{Results}

The full set of raw data, metadata, and plots are available at this \href{https://drive.google.com/drive/folders/15gt9bv0kPBCHnJyW_RzsYQygeMaK1uCf?usp=sharing}{link} (warning: approximately 700 MB). For all our experiments, our primary measure of policy performance was the mean reward over the second half of the episodes, with a rolling mean over the previous 5\% of the total number of episodes providing a similar but smoother representation of model performance.

\subsection{Random Policy}

As a baseline, we ran a random policy on each game, with results in Figures \ref{fig:CartPole_random}, \ref{fig:Acrobot_random}, and \ref{fig:MountainCar_random}. The average rewards and durations for the three games are available in Table \ref{tab:random_rewards}. With this initial goal in mind, we proceeded to test different model architectures and hyperparameter settings to achieve good empirical results over all three games.

\foreach \game in {CartPole, Acrobot, MountainCar}
{
    \begin{figure*}[!ht]
        \centering
        \subfloat[Rewards by episode, as well as a rolling mean, for a random policy in the \texttt{\game}~game. ]{%
            \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_rewards.pdf}
            \label{subfig:\game_NoTraining_Random_rewards}
        }
        \hfill
        \subfloat[Durations by episode, as well as a rolling mean, for a random policy in the \texttt{\game}~game.]{%
            \includegraphics[width=0.45\textwidth]{\game/\game_NoTraining_Random_durations.pdf}
            \label{subfig:\game_NoTraining_Random_duraions}
        }
    \caption{Baseline results from the \texttt{\game}~game from OpenAI Gym under a random policy.}
    \label{fig:\game_random}
    \end{figure*}
}

\begin{table*}[!ht]
    \footnotesize
    \centering
    \begin{tabular}{c|cc}
        \toprule
        Game & Average Duration & Average Reward \\ \midrule
        CartPole-v0 & 18.441 & 18.441 \\
        Acrobot-v1 & 599.73 & -598.73 \\
        MountainCar-v0 & 2717.77 & -2717.77 \\
        \bottomrule
    \end{tabular}
    \caption{Average rewards and durations under a random policy for the three games.}
    \label{tab:random_rewards}
\end{table*}

% \subsection{Initial Exploration}

% Although \cite{mnih2013playing, mnih2015human} used a learning rate of $0.00025$, we found that learning was far too slow for our purposes and instead opted for a larger learning rate. We began with a learning rate of $0.1$, but this was far too large: CartPole would learn to durations of around 200 time steps within a few dozen iterations but would quickly diverge. The other two games also did not converge. Significant improvements occurred after reducing to a learning rate of about $0.001$. 

% We used

% \begin{itemize}
%     \item \cp~learning rate 001
% \end{itemize}

\subsection{Hyperparameter Tuning Results}

In our hyperparameter tuning, we decided to keep the frame skip, update frequency, replay memory size, and agent action selection hyperparameters constant for testing, since grid search sizes increase exponentially in the number of parameters, and varying these settings would vastly increase simulation time and space consumption. 

All summary statistics are taken over the second half of the experiment, after the algorithm has had time to learn the game and converge. This prevents the learning process from affecting measurements of the final learned policy.

\subsubsection{First Grid Search (10,000 Iterations)}

In order to establish a solid set of hyperparameters to which PCA variants of DQNs could be compared, we began our experiments with an initial grid search over the hyperparameters of interest for 10,000 training steps to eliminate poorly performing hyperparameter settings. Table \ref{tab:grid_search_v1_10k} contains the results of the top fifteen models for each game ordered by performance.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v1_10k.tex}
    
    \caption{Top fifteen parameter-tuned mean rewards per game for our first grid search. Each run used either the DDQN-GS or DQN-GS model and lasted for 10,000 training steps. Note that DDQNs do not have a target update option, so their value in the ``target'' column is N/A.}
    \label{tab:grid_search_v1_10k}
\end{table*}

From these results, we already find that our best hyperparameter configurations beat the random baseline. \mc~was rather difficult to learn, as the final policy seems to depend on the random initialization. With sub-optimal hyperparameters, the episodes under a poorly learned policy can extend to tens of thousands of time steps, far more than a random policy. This particular vulnerability to divergence likely occurs because all transitions entered into the replay memory have $r = -1$, creating difficulties in learning. The average episode in \cp~roughly doubles the duration of the random policy, though the performance has significantly high variance in all models, reaching maxima of up to 200 time steps. Finally, \ab~seems to have been learned quite quickly, reaching optimal performance within the 10,000 training steps.

Here, we observe consistently strong performance by DDQN-GSs over DQN-GSs and by using a Huber loss function over an MSE loss. Other optimizers, such as Adam, failed to learn in pre-testing, so we used the RMSProp optimizer (as suggested by \cite{mnih2013playing})\todo{discuss why we used rmsprop?} throughout our experiments. Weight decay yielded mixed results, likely due to the fact that we only allowed the models to run for 10,000 training steps. Additionally, the effectiveness of the learning rate parameters varied significantly with the game; for example, \cp~learns better with a significantly higher learning rate than the other two games. We also saw that annealing the learning rate slightly improved the overall performance of the algorithms, so we decided to keep the annealing for subsequent experiments. In theory, a learning rate annealing should improve the stability of the model after the model has had time to learn a strong policy. Moreover, not displayed in the data is the surprising fact that DQN-GSs with an enabled target update performed significantly worse than DQN-GSs without a target update in that they failed to reach any convergence at all, so we also decided to remove target update functionality for future experiments. With these observations in mind, we proceeded to test another batch of models for a longer period. 

\subsubsection{Second Grid Search (50,000 Iterations)}
 
Setting the optimizer, loss function, and target update to RMSProp, Huber loss, and False, respectively, we performed a second grid search over 50,000 training iterations to tune the learning rate and weight decay. The results can be found in Table \ref{tab:grid_search_v2_50k}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v2_50k.tex}
    
    \caption{Top fifteen parameter-tuned mean rewards per game for our second grid search. All experiments used a Huber loss, learning rate annealing, no target update, and the RMSProp optimizer for 50,000 training steps.}
    \label{tab:grid_search_v2_50k}
\end{table*}

\mc~and \ab~continued to perform well with more training time for the best models, though we began to see evidence of high variance in performance between models for these games. Both had the worst models getting average scores near the level of randomness. In these games, episodes are not completed until the agent succeeds at reaching above a certain height or climbing up the mountain. As a result, if an agent learns a poor policy early on, the agent could spend all of its training iterations receiving $-1$ reward with no positive feedback. To help mitigate this issue, we capped the number of training iterations per episode to 20,000 with the intent to expose the agent to more positive states and to speed up learning time. If an agent reaches a duration of 20,000, we already know that the agent does a poor job of learning a good policy and can afford to ignore the model, since in our random policy experiments, episode durations never surpassed 16,000 and rarely lasted more than 10,000.

Again, we see that DDQN-GSs tend to outperform DQN-GSs. The learning rate, however, is inconsistent from the first grid search. Here, we find that overall, lower learning rates are preferred over higher ones for longer grid search durations. This is reasonable, since we expect that with a longer training duration, the agent has more time to converge to a good policy, whereas with only 10,000 iterations, the model needs a higher learning rate to quickly outdo random performance. As a result, we decided to continue even higher training times with lower learning rates. Weight decay does not appear to have a significant impact on the performance of the models, so we decided to take the more conservative option of a regularization coefficient of $0.1$.

\subsubsection{Third Grid Search (100,000 Iterations)}

By this point, we established that DDQNs tended to outperform DQNs, so we then explored hyperparameter configurations that yielded good performance over 100,000 iterations. We then performed a grid search, varying the batch size and learning rate. The results can be found in \ref{tab:grid_search_v3_100k}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/grid_search_v3_100k.tex}
    
    \caption{Complete results for our third grid search, sorted by mean reward. All experiments used the DDQN-GS model with a Huber loss function, learning rate annealing, and a $0.1$ weight decay for 100,000 training iterations.}
    \label{tab:grid_search_v3_100k}
\end{table*}

The data here indicate mixed results. \mc~tends to do better with larger batch sizes, while \ab~and \cp~do better with a smaller batch size. The learning rates differed from game to game as well. With no significant trends in performance, we opted to use a batch size of 128 to increase learning speed and a learning rate of $0.001$ to prevent divergence. Our final hyperparameters, used in the rest of the paper, are available in Table \ref{tab:grid_search_hyperparameters}.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \begin{tabular}{c|c}
        \toprule
        Hyperparameter & Value \\ \midrule
        Model & DDQN \\
        Frame skip & $3$ \\
        Update frequency & $4$ \\
        Training updates & $100000$ \\
        Replay memory size & $10000$ \\
        Target update & N/A \\
        Learning rate & $0.001$ \\
        Learning rate annealing & Yes \\
        Batch size & $128$ \\
        Loss function & Huber loss \\
        Regularization & $0.1$ \\
        \bottomrule
    \end{tabular}
    \caption{The hyperparameters found by our grid search and used for the PCA variants of the (D)DQN.}
    \label{tab:grid_search_hyperparameters}
\end{table*}

\subsection{PCA Networks}

With a strong DDQN-GS model selected as our new baseline, we then began to test the performance of PCA-based models. These model comparison results generalized quite well to other hyperparameter settings. See Figures \ref{fig:CartPole_DDQN-GS_rewards_losses} to \ref{fig:MountainCar_DDQCNN-PCA-Mini_rewards_losses} for a graphical comparison of the PCA networks tested. Table \ref{tab:final_v1_100k} contains the results for the plots.

\foreach \game in {CartPole, Acrobot, MountainCar}
{   
    \foreach \model in {DDQN-GS, DDQN-PCA, DDQCNN-PCA, DDQCNN-PCA-Mini}
    {
        \begin{figure*}[!ht]
            \centering
            \subfloat[Rewards by episode from training the \model~model on the \texttt{\game}~game. \label{subfig:\game_\model_EpsilonGreedy_rewards_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_rewards_cpu.pdf}
            }
            \hfill
            \subfloat[Losses by number of training steps from training the \model~model on the \texttt{\game}~game. \label{subfig:\game_\model_EpsilonGreedy_losses_cpu}]{%
                \includegraphics[width=0.45\textwidth]{\game/\game_\model_EpsilonGreedy_losses_cpu.pdf}
            }
            \caption{Rewards and Huber losses for the \model~model in the \texttt{\game}~game over 100,000 minibatch training steps.}
            \label{fig:\game_\model_rewards_losses}
        \end{figure*}
    }
}

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/final_v1_100k.tex}
    
    \caption{Model comparison with final hyperparameters over 100,000 training iterations.}
    \label{tab:final_v1_100k}
\end{table*}

First, we note that the (D)DQN-PCA model, which does not convolve over an image representation, does relatively poorly in all settings. This is reasonable. Since this model does not account for the structural properties of adjacent pixels as a convolutional neural network does, the model loses a significant amount of information about the image. We then turn our attention to the convolutional PCA versions of the models.

The DDQCNN-PCA model performed surprisingly well for \cp~and \ab, outperforming the DDQN-GS models and generating some of the best scores we ever saw in our experiments. One possible reason for the superior performance is the fact that PCA simplifies the images, thereby reducing noise and allowing for more efficient learning. Unfortunately, these results were not replicated in the \mc~game, failing to beat even the random baseline. It seems that \mc~is sensitive to weight initialization, causing unpredictable fluctuations in its performance.

What is more surprising is the performance of DDQCNN-PCA-Mini model, being a strong performer in the \mc~setting. We had no expectations that this model would do well. After all, the PCA simply selects the most prominent features in the projected subspace and are \textit{ad hoc} formed into a square image to convolve over without any consideration of the structural integrity of the original image. It may be that the projection yielded features in the reduced space that had high correlation between one another and thus high predictive power, but otherwise, we have little explanation for the model's performance and mention it only as a point of interest for future work. 

Despite some PCA variants being high performers in each of the games, many of the remaining PCA models were unable to perform above a random baseline. Upon closer inspection of the reward graphs, we find that several PCA models reached a strong optimum around the middle of the training process, but ended up diverging over time. Overall, we find that PCA is sensitive to divergence, but less so than the original model; additionally, under the right use cases, the right learning rate annealing, and $\epsilon$ decay, PCA is a promising method to increase the performance of a Q-network under short learning time settings.

\subsection{Testing Layer Sizes}

We wanted to be sure that our DDQN-PCA model's performance was not heavily impacted by the network's architecture. We therefore tested different network architectures for the DDQN-PCA model to see whether there was a significant difference in performance over 100,000 training iterations. See Table \ref{tab:model_selection_ddqn_pca_4} for the results from testing several conventional deep neural network architectures for the DDQN-PCA model.

\begin{table*}[!ht]
    \footnotesize
    \centering
    \input{csvs/model_selection_ddqn_pca_4.tex}
    
    \caption{Results for from testing several conventional deep neural network architectures for the DDQN-PCA model. All experiments used a Huber loss function, learning rate annealing, a batch size of $128$, a learning rate of $0.001$, and a $0.1$ weight decay for 100,000 training iterations.}
    \label{tab:model_selection_ddqn_pca_4}
\end{table*}

Unfortunately, despite trying different numbers of layers as well as the number of nodes at each layer, none of the models particularly stood out as a better model than the ones we tried in our final comparison.

\section{Discussion}

Throughout this exploratory project, we found that learning to play games from image data alone makes each problem significantly harder. While we had the option to read in metadata about each state such as the pole's angle or the car's velocity, we decided that the best way to standardize the performance of our model was to ensure that each game fed in the same type of data, namely pixels. Note that there are many simulations online that claim to learn better scores for \cp~even just by using pixel data, but these simulations take advantage of cropping the whitespace around the cart, drastically improving the performance of the convolution.

One thing not discussed in great detail was how we decided upon which parameters we chose at every step. In addition to empirical evaluations, we also built generalized linear models on our hyperparameters, treating them as categorical variables and predicting the mean. By examining the coefficients of the model, this gave us a rough estimate of how different features were correlated with the mean reward of each model.

An interesting problem that we encountered while testing different models was that there was little correlation between the loss function and the performance of the model. Indeed, more often than not, the loss function continuously shot up as the simulations ran without ever converging toward an actual minimization. Typically, this indicates that we are underfitting our model, since the model is never able to minimize the loss; however, online discussions claim that this is not a problem in a reinforcement learning setting \cite{stackexchange2017loss}.

During our modeling, we tried keeping track of sample Q-values as the model trained. We randomly sampled a set of 128 states from the replay memory and determined the average Q-value over all states after each episode Over time, we expected the sample Q-values to increase as the policy improved, and the expected future return increased. While the Q-value did tend to increase for the sample, the values tended to explode to unreasonably large numbers for \cp. This seemed to be a sign that our weights were exploding in our neural network, but even after clamping our nodes' values to be between $-1$ and $1$, we were unable to get more reasonable estimates of sample Q-values. 

\section{Conclusion}

To conclude, we have built up a robust coding framework to test different Q-function models on a variety of discrete-space OpenAI games. The results seem to show that, though PCA is prone to diverge from optimal performance at times, it appears to also have a reasonable chance to do well in game settings. Perhaps with the right model regularization and network architecture, PCA is a promising way to increase model performance for reinforcement learning by reducing the state space and eliminating the need for the DQN to learn unimportant feature information.

\subsection{Further Directions}

Further directions include performing the same PCA analysis for DQN performance on other games, particularly Atari games. Games with large (but discrete) action spaces are also good candidates, since the games used in our experiments had only two or three actions. Additionally, extensions to games with continuous action spaces, which would likely need a significantly different network architecture (such as a convolutional neural network linked with a network in which actions are the inputs), pose an interesting problem for DQNs, whose network structure relies heavily on the presence of a discrete action space.

We had severely limited computational power and time, so extending this work using more varied learning rates and annealing schedules could offer better performance. Also, computing using GPUs (which were unfortunately not available) rather than CPUs would have been a substantial time boost.

Recent developments and additions to DQNs also suggest promising improvements to the general DQN framework as well as the use of PCA in reinforcement learning. For instance, adding hindsight experience replay \cite{andrychowicz2017hindsight} or prioritized experience replay \cite{schaul2015prioritized}, particularly to \mc, which is heavily dependent on choosing the right transitions in each minibatch update, would further improve our algorithms' performance and check the robustness of using PCA with slightly different learning algorithms.

\newpage
\bibliography{final_project}{}
\bibliographystyle{plain}

\onecolumn

\appendix

\section{Setup Instructions}
\label{app:getting_started}

\begin{enumerate}
    \item Download/install Anaconda
    
    \item Clone this paper's repository: \url{https://github.com/hahakumquat/stat234-project}
    
    \item Inside the directory containing the repository, create and activate the \texttt{conda} environment, which consists of all the necessary packages: 
    
    \begin{minted}{bash}
    conda env create -f environment.yml
    source activate stat234
    \end{minted}

    \item Fix errors if unmerged into master OpenAI Gym branch
    
    \begin{itemize}
        \item Consistent with the pull request at \url{https://github.com/openai/gym/pull/972}, in each of the game files in \path{gym/gym/envs/classic_control/}, add \texttt{dtype=np.float32} to each \texttt{spaces.Box()} initialization to suppress the logger warning
    \end{itemize}
    
    \item Run \texttt{main.py}
    
    \begin{itemize}
        \item \texttt{python main.py -h} for command-line argument instructions and a full list of arguments and defaults.
        \item \texttt{python main.py -g CartPole-v0 -m DQN\_GS -e 1000} for training a normal grayscale DQN on \cp~ for 1000 training steps
        \item \texttt{python main.py -g Acrobot-v1 -m DDQCNN\_PCA} for training a convolutional PCA variant of a DDQN
        \item \texttt{python main.py -g CartPole-v0 -a Random -e 1000} for a random policy
    \end{itemize}
\end{enumerate}

\todo{add additional plots or data to appendix}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
